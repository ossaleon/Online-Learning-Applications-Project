{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for GP\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, DotProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UserClass import *\n",
    "from basic_environments import *\n",
    "from basic_learners import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DATA_users.py\n",
    "%run DATA_parameters.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0)], [(0, 1)], [(1, 0)]]\n"
     ]
    }
   ],
   "source": [
    "user_classes = [C1, C2, C3]\n",
    "real_contexts = [[(uc.F1, uc.F2)] for uc in user_classes ]\n",
    "print(real_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obs: it is the same environment of step3, with self.user_class commented\n",
    "class ContextEnvironment:\n",
    "    def __init__(self, bids, prices, sigma_clicks, sigma_costs, margin_param, userclass):\n",
    "        # self.userclass = userclass\n",
    "\n",
    "        self.bidding_environment = BiddingEnvironment(bids, sigma_clicks, sigma_costs, userclass)\n",
    "        self.pricing_environment = PricingEnvironment(prices, margin_param, userclass)\n",
    "\n",
    "\n",
    "    def round(self, pulled_arm_bid, pulled_arm_price):\n",
    "        n_daily_clicks, cum_daily_costs = self.bidding_environment.round(pulled_arm_bid)\n",
    "        converted_clicks, reward = self.pricing_environment.round_step3(pulled_arm_price, n_daily_clicks, cum_daily_costs)\n",
    "\n",
    "        return n_daily_clicks, cum_daily_costs, converted_clicks, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Environment:\n",
    "#     def __init__(self, bids, prices, sigma_clicks, sigma_costs, margin_param, user_classes):\n",
    "#         self.user_classes = user_classes\n",
    "#         self.n_classes = len(user_classes)\n",
    "\n",
    "#         self.single_environments = [ContextEnvironment(bids, prices, sigma_clicks, sigma_costs, margin_param, user_class) for user_class in user_classes]\n",
    "#         self.features_list = [(user_class.F1,user_class.F2) for user_class in user_classes]\n",
    "\n",
    "#     def round(self, pulled_arm_bid_list, pulled_arm_price_list, partition = [ [(0,0),(0,1),(1,0),(1,1)] ]):\n",
    "#         \"\"\"\n",
    "#         pulled_arm_bid_list: list of integers\n",
    "#             idx of the pulled arm ob the bids for each component of the partition\n",
    "\n",
    "#         pulled_arm_price_list: list of integers\n",
    "#             idx of the pulled arm ob the prices for each component of the partition\n",
    "\n",
    "#         partition: list of lists of tuples of integers\n",
    "#             notation: each element must contain a tuple indicating the partitioning for the features F1 and F2\n",
    "#             e.g.\n",
    "#                 if no partition ((the same as one set containing all possible features in the partition)): partition = [ [(0,0),(0,1),(1,0),(1,1)] ]\n",
    "#                 if partition by splitting F1: partition = [ [(0,0),(0,1)] , [(1,0),(1,1)] ]\n",
    "\n",
    "#         Note: pulled_arm_bid_list, pulled_arm_price_list, partition must have the same length\n",
    "#         \"\"\"\n",
    "\n",
    "#         n_daily_clicks_list = []\n",
    "#         cum_daily_costs_list = []\n",
    "#         converted_clicks_list = []\n",
    "#         reward_list = []\n",
    "#         for i in range(self.n_classes):\n",
    "#             user_F1 = self.user_classes[i].F1\n",
    "#             user_F2 = self.user_classes[i].F2\n",
    "\n",
    "#             idx_context = 0\n",
    "#             for context in partition:\n",
    "#                 if (user_F1,user_F2) in context:\n",
    "#                     break\n",
    "#                 idx_context += 1\n",
    "\n",
    "#             n_daily_clicks, cum_daily_costs, converted_clicks, reward = self.single_environments[i].round(pulled_arm_bid_list[idx_context],\n",
    "#                                                                                                           pulled_arm_price_list[idx_context])\n",
    "\n",
    "\n",
    "#             n_daily_clicks_list.append(n_daily_clicks)\n",
    "#             cum_daily_costs_list.append(cum_daily_costs)\n",
    "#             converted_clicks_list.append(converted_clicks)\n",
    "#             reward_list.append(reward)\n",
    "\n",
    "#         return n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, self.features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, bids, prices, sigma_clicks, sigma_costs, margin_param, user_classes):\n",
    "        self.user_classes = user_classes\n",
    "        self.n_classes = len(user_classes)\n",
    "        self.user_environments = [ContextEnvironment(bids, prices, sigma_clicks, sigma_costs, margin_param, user_class) for user_class in user_classes]\n",
    "        self.features_list = [(user_class.F1,user_class.F2) for user_class in user_classes]\n",
    "        \n",
    "        \n",
    "    def round(self, pulled_arm_bid_list, pulled_arm_price_list, pulled_contexts):\n",
    "        \"\"\"\n",
    "        pulled_arm_bid_list: list of integers\n",
    "            idx of the pulled arm ob the bids for each component of the partition\n",
    "        pulled_arm_price_list: list of integers\n",
    "            idx of the pulled arm ob the prices for each component of the partition\n",
    "        pulled_contexts: list of lists of tuples of integers\n",
    "            notation: each element must contain a tuple indicating the partitioning for the features F1 and F2\n",
    "            e.g.\n",
    "                if no partition ((the same as one set containing all possible features in the partition)): partition = [ [(0,0),(0,1),(1,0),(1,1)] ]\n",
    "                if partition by splitting F1: partition = [ [(0,0),(0,1)] , [(1,0),(1,1)] ]\n",
    "        Note: pulled_arm_bid_list, pulled_arm_price_list, partition must have the same length\n",
    "        About output: if pulled_contexts is valid (and is a partition), the length of each element in the output is the number of user classes \n",
    "        \"\"\"\n",
    "        \n",
    "        n_daily_clicks_list = []\n",
    "        cum_daily_costs_list = []\n",
    "        converted_clicks_list = []\n",
    "        reward_list = []\n",
    "        for user, user_env in zip(self.user_classes, self.user_environments):\n",
    "            for pulled_arm_bid, pulled_arm_price, context in zip(pulled_arm_bid_list, pulled_arm_price_list, pulled_contexts):\n",
    "                n_daily_clicks, cum_daily_costs, converted_clicks, reward = user_env.round(pulled_arm_bid, pulled_arm_price)\n",
    "\n",
    "                \n",
    "                if (user.F1,user.F2) in context:\n",
    "                    n_daily_clicks_list.append(n_daily_clicks)\n",
    "                    cum_daily_costs_list.append(cum_daily_costs)\n",
    "                    converted_clicks_list.append(converted_clicks)\n",
    "                    reward_list.append(reward)\n",
    "                    break\n",
    "\n",
    "        if len(n_daily_clicks_list) != 3:########################################################################## TO DELETE FOR PRESENTATION\n",
    "            print(\"SOMETHING WRONG: LOOK if you are using correctly class environment\")\n",
    "            \n",
    "        return n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, self.features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learners\n",
    "\n",
    "hp:\n",
    "- we assume that the features we observe are binary  \n",
    "    e.g. if we have 2 features: (0,0), (0,1), (1,0), (1,1) are the all the possible observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Com: It is like of the Custom_S3_UCB_Learner with the following modifications:\n",
    "\n",
    "Modifications:\n",
    "- new attribute: context\n",
    "- new method: update_from_batch\n",
    "\"\"\"\n",
    "#\n",
    "## Convertion rate version ##\n",
    "#\n",
    "# class UCB_SingleContextLearner(Learner):# Obs: the curve of n_clicks and cost are always the same for any price we put, and the conversion rate is always the same for any bid we put\n",
    "#     def __init__(self, arms_bids, arms_prices, margin_param, context, M = 1):\n",
    "#         n_arms_bids = len(arms_bids)\n",
    "#         n_arms_prices = len(arms_prices)\n",
    "#         super().__init__(0)\n",
    "#         self.rewards_per_arm = [[[] for i in range(n_arms_prices)] for j in range(n_arms_bids)] # Notation: reward is in rewards_per_arm[idx_bid][idx_price]\n",
    "\n",
    "#         self.arms_prices = arms_prices\n",
    "#         self.arms_bids = arms_bids\n",
    "\n",
    "#         self.margin_param = margin_param\n",
    "\n",
    "#         #learners:\n",
    "#         self.n_daily_clicks_learner = GPUCB_Learner(arms_bids)\n",
    "#         self.cum_daily_costs_learner = GPUCB_Learner(arms_bids)\n",
    "#         self.conversion_rate_learner = UCB1_Learner(n_arms_prices, (0,M))\n",
    "\n",
    "#         self.context = context# it is a list of tuples #\n",
    "\n",
    "\n",
    "#     def pull_arm(self):\n",
    "#         \"\"\"\n",
    "#         Notes:\n",
    "#             The higher the conversion rate the higher the reward => I can pull directly the an arm of the prices for the highest alpha,\n",
    "#             while the clicks and costs I have to evaluate them together and pull the bid that maximizes the reward\n",
    "\n",
    "#             Question:\n",
    "#                 Once I know the price arm to be pulled, for the bid arm, should I consider the upperbound of the conversion rate or just the mean of the conversion rate?\n",
    "#                 -   choosing the mean of the conversion rate, there is just the uncertainty of related to the bid and the advertising curves\n",
    "#                 -   choosing the upperbound of the conversion rate, it adds uncertainty related to the problem \"conversion_rate(price)\"\n",
    "#                 --> I'm using the upperbound choice [but still an open question]\n",
    "#         \"\"\"\n",
    "#         ub_clicks = self.n_daily_clicks_learner.means + self.n_daily_clicks_learner.sigmas\n",
    "#         lb_costs = self.cum_daily_costs_learner.means - self.cum_daily_costs_learner.sigmas\n",
    "#         ub_conversion_rates = self.conversion_rate_learner.empirical_means + self.conversion_rate_learner.confidence\n",
    "\n",
    "#         ub = ub_conversion_rates * (self.arms_prices - self.margin_param)\n",
    "#         idx_price_arm = np.random.choice(np.where(ub == ub.max())[0])\n",
    "\n",
    "#         ub_reward = ub_clicks * ub[idx_price_arm] - lb_costs\n",
    "#         idx_bid_arm = np.random.choice(np.where(ub_reward == ub_reward.max())[0])\n",
    "\n",
    "#         return (idx_bid_arm, idx_price_arm)\n",
    "    \n",
    "\n",
    "#     def update_observations(self, pulled_arm_bid, pulled_arm_price, reward):\n",
    "#         self.rewards_per_arm[pulled_arm_bid][pulled_arm_price].append(reward)\n",
    "#         self.collected_rewards.append(reward)\n",
    "\n",
    "\n",
    "#     def update(self, pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward):\n",
    "#         \"\"\"\"\n",
    "#         Obs: if n_daily_clicks = 0, we do not update the conversion_rate_learner\n",
    "#         \"\"\"\n",
    "#         self.t += 1\n",
    "#         self.update_observations(pulled_arm_bid, pulled_arm_price, reward)\n",
    "\n",
    "#         self.n_daily_clicks_learner.update(pulled_arm_bid, n_daily_clicks)\n",
    "#         self.cum_daily_costs_learner.update(pulled_arm_bid, cum_daily_costs)\n",
    "\n",
    "#         if n_daily_clicks > 0:\n",
    "#             alpha = converted_clicks / n_daily_clicks\n",
    "#             self.conversion_rate_learner.update(pulled_arm_price, alpha)\n",
    "#         else:\n",
    "#             self.conversion_rate_learner.t += 1# need to be increased, because t is used in the computation of the upperbound\n",
    "\n",
    "    \n",
    "#     def update_from_batch(self, pulled_arm_bid_data, pulled_arm_price_data, n_daily_clicks_data, cum_daily_costs_data, converted_clicks_data, reward_data, time):######\n",
    "#         \"\"\"Obs:\n",
    "#             - not all rewards are valid to update the convertion rate, because n_clicks can be 0 in some of values in n_daily_clicks_data\n",
    "#         \"\"\"\n",
    "#         arms_of_alphas = []\n",
    "#         alphas = []\n",
    "#         for i, reward in enumerate(reward_data):\n",
    "#             self.update_observations(pulled_arm_bid_data[i], pulled_arm_price_data[i], reward)\n",
    "#             if n_daily_clicks_data[i] > 0:\n",
    "#                 arms_of_alphas.append(pulled_arm_price_data[i])\n",
    "#                 alphas.append(converted_clicks_data[i] / n_daily_clicks_data[i])\n",
    "\n",
    "#         self.conversion_rate_learner.update_from_batch(arms_of_alphas, alphas, time)\n",
    "#         self.n_daily_clicks_learner.update_from_batch(pulled_arm_bid_data, n_daily_clicks_data, time)\n",
    "#         self.cum_daily_costs_learner.update_from_batch(pulled_arm_bid_data, cum_daily_costs_data, time)\n",
    "#         self.t += time\n",
    "\n",
    "\n",
    "\n",
    "class UCB_SingleContextLearner(Learner):# Obs: the curve of n_clicks and cost are always the same for any price we put, and the conversion rate is always the same for any bid we put\n",
    "    def __init__(self, arms_bids, arms_prices, margin_param, context, M = 1):\n",
    "        n_arms_bids = len(arms_bids)\n",
    "        n_arms_prices = len(arms_prices)\n",
    "        super().__init__(0)\n",
    "        self.rewards_per_arm = [[[] for i in range(n_arms_prices)] for j in range(n_arms_bids)] # Notation: reward is in rewards_per_arm[idx_bid][idx_price]\n",
    "\n",
    "        self.arms_prices = arms_prices\n",
    "        self.arms_bids = arms_bids\n",
    "\n",
    "        self.margin_param = margin_param\n",
    "\n",
    "        #learners:\n",
    "        self.n_daily_clicks_learner = GPUCB_Learner(arms_bids)\n",
    "        self.cum_daily_costs_learner = GPUCB_Learner(arms_bids)\n",
    "        # self.conversion_rate_learner = UCB1_Learner(n_arms_prices, (0,1))\n",
    "        self.expected_margin_learner = UCB1_Learner(n_arms_prices, (0,M))\n",
    "\n",
    "        self.context = context# it is a list of tuples #\n",
    "\n",
    "\n",
    "    def pull_arm(self):\n",
    "        \"\"\"\n",
    "        Notes:\n",
    "            The higher the conversion rate the higher the reward => I can pull directly the an arm of the prices for the highest alpha,\n",
    "            while the clicks and costs I have to evaluate them together and pull the bid that maximizes the reward\n",
    "\n",
    "            Question:\n",
    "                Once I know the price arm to be pulled, for the bid arm, should I consider the upperbound of the conversion rate or just the mean of the conversion rate?\n",
    "                -   choosing the mean of the conversion rate, there is just the uncertainty of related to the bid and the advertising curves\n",
    "                -   choosing the upperbound of the conversion rate, it adds uncertainty related to the problem \"conversion_rate(price)\"\n",
    "                --> I'm using the upperbound choice [but still an open question]\n",
    "        \"\"\"\n",
    "        ub_clicks = self.n_daily_clicks_learner.means + self.n_daily_clicks_learner.sigmas\n",
    "        lb_costs = self.cum_daily_costs_learner.means - self.cum_daily_costs_learner.sigmas\n",
    "        ub_expected_margin = self.expected_margin_learner.empirical_means + self.expected_margin_learner.confidence\n",
    "\n",
    "        idx_price_arm = np.random.choice(np.where(ub_expected_margin == ub_expected_margin.max())[0])\n",
    "\n",
    "        # ub_reward = ub_clicks * ub[idx_price_arm] - lb_costs\n",
    "        ub_reward = ub_clicks * ub_expected_margin[idx_price_arm] - lb_costs\n",
    "        idx_bid_arm = np.random.choice(np.where(ub_reward == ub_reward.max())[0])\n",
    "\n",
    "        return (idx_bid_arm, idx_price_arm)\n",
    "    \n",
    "\n",
    "    def update_observations(self, pulled_arm_bid, pulled_arm_price, reward):\n",
    "        self.rewards_per_arm[pulled_arm_bid][pulled_arm_price].append(reward)\n",
    "        self.collected_rewards.append(reward)\n",
    "\n",
    "\n",
    "    def update(self, pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward):\n",
    "        \"\"\"\"\n",
    "        Obs: if n_daily_clicks = 0, we do not update the conversion_rate_learner\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        self.update_observations(pulled_arm_bid, pulled_arm_price, reward)\n",
    "\n",
    "        self.n_daily_clicks_learner.update(pulled_arm_bid, n_daily_clicks)\n",
    "        self.cum_daily_costs_learner.update(pulled_arm_bid, cum_daily_costs)\n",
    "\n",
    "        if n_daily_clicks > 0:\n",
    "            expected_margin = (converted_clicks / n_daily_clicks) * (self.arms_prices[pulled_arm_price] - self.margin_param)\n",
    "            self.expected_margin_learner.update(pulled_arm_price, expected_margin)\n",
    "        else:\n",
    "            self.expected_margin_learner.t += 1# need to be increased, because t is used in the computation of the upperbound\n",
    "\n",
    "    \n",
    "    def update_from_batch(self, pulled_arm_bid_data, pulled_arm_price_data, n_daily_clicks_data, cum_daily_costs_data, converted_clicks_data, reward_data, time):######\n",
    "        \"\"\"Obs:\n",
    "            - not all rewards are valid to update the convertion rate, because n_clicks can be 0 in some of values in n_daily_clicks_data\n",
    "        \"\"\"\n",
    "        arms_of_alphas = []\n",
    "        alphas = []\n",
    "        for i, reward in enumerate(reward_data):\n",
    "            self.update_observations(pulled_arm_bid_data[i], pulled_arm_price_data[i], reward)\n",
    "            if n_daily_clicks_data[i] > 0:\n",
    "                arms_of_alphas.append(pulled_arm_price_data[i])\n",
    "                alphas.append((converted_clicks_data[i] / n_daily_clicks_data[i]) * (self.arms_prices[pulled_arm_price_data[i]] - self.margin_param))\n",
    "\n",
    "        self.expected_margin_learner.update_from_batch(arms_of_alphas, alphas, time)\n",
    "        self.n_daily_clicks_learner.update_from_batch(pulled_arm_bid_data, n_daily_clicks_data, time)\n",
    "        self.cum_daily_costs_learner.update_from_batch(pulled_arm_bid_data, cum_daily_costs_data, time)\n",
    "        self.t += time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB_FixedContextsLearner(Learner):\n",
    "    def __init__(self, arms_bids, arms_prices, margin_param, contexts, M = 1):\n",
    "        super().__init__(0)# This learner learns by context and not by arms directly (the arms are inside the context_learners).\n",
    "\n",
    "        self.collected_n_daily_clicks = []\n",
    "        self.collected_cum_daily_costs = []\n",
    "        self.collected_converted_clicks = []\n",
    "        self.collected_features = []\n",
    "\n",
    "        self.n_arms_bids = len(arms_bids)\n",
    "        self.n_arms_prices = len(arms_prices)\n",
    "        self.arms_prices = arms_prices\n",
    "        self.arms_bids = arms_bids\n",
    "\n",
    "        self.margin_param = margin_param\n",
    "        self.contexts = contexts\n",
    "\n",
    "        # Learners:\n",
    "        self.context_learners = [UCB_SingleContextLearner(arms_bids, arms_prices, margin_param, context, M) for context in self.contexts]\n",
    "\n",
    "\n",
    "    def pull_arm(self):\n",
    "        idx_bid_arm_list = []\n",
    "        idx_price_arm_list = []\n",
    "        for context_learner in self.context_learners:\n",
    "            idx_bid_arm, idx_price_arm = context_learner.pull_arm()\n",
    "            idx_bid_arm_list.append(idx_bid_arm)\n",
    "            idx_price_arm_list.append(idx_price_arm)\n",
    "\n",
    "        return (idx_bid_arm_list, idx_price_arm_list, self.contexts)\n",
    "\n",
    "\n",
    "    def update_observations(self, n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list):\n",
    "        # no self.rewards_per_arm[pulled_arm].append(reward) because no concept of an arm (the arms are used inside the context learners)\n",
    "        self.collected_rewards.append(reward_list)\n",
    "        self.collected_n_daily_clicks.append(n_daily_clicks_list)\n",
    "        self.collected_cum_daily_costs.append(cum_daily_costs_list)\n",
    "        self.collected_converted_clicks.append(converted_clicks_list)\n",
    "        self.collected_features.append(features_list)\n",
    "        \n",
    "\n",
    "    def update(self, pulled_arm_bid_list, pulled_arm_price_list,\n",
    "               n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list):\n",
    "        \"\"\"Obs: the length of the lists of the pulled arms are not necessary equal to those in the lists of clicks, costs, rewards and features\"\"\"\n",
    "        self.t += 1\n",
    "        self.update_observations(n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list)\n",
    "\n",
    "        for i,context in enumerate(self.contexts):\n",
    "            pulled_arm_bid_data = []\n",
    "            pulled_arm_price_data = []\n",
    "\n",
    "            n_daily_clicks_data = []\n",
    "            cum_daily_costs_data = []\n",
    "            converted_clicks_data = []\n",
    "            reward_data = []\n",
    "\n",
    "            for j,fts in enumerate(features_list):\n",
    "                if fts in context:\n",
    "                    pulled_arm_bid_data.append(pulled_arm_bid_list[i])\n",
    "                    pulled_arm_price_data.append(pulled_arm_price_list[i])\n",
    "\n",
    "                    n_daily_clicks_data.append(n_daily_clicks_list[j])\n",
    "                    cum_daily_costs_data.append(cum_daily_costs_list[j])\n",
    "                    converted_clicks_data.append(converted_clicks_list[j])\n",
    "                    reward_data.append(reward_list[j])\n",
    "\n",
    "            self.context_learners[i].update_from_batch(pulled_arm_bid_data, pulled_arm_price_data,\n",
    "                                                       n_daily_clicks_data, cum_daily_costs_data, converted_clicks_data, reward_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB_ContextsGreadyLearner(UCB_FixedContextsLearner):\n",
    "    def __init__(self, arms_bids, arms_prices, margin_param, window_context, M = 1): \n",
    "        # self.n_features = 2\n",
    "        contexts = [ [(0,0),(0,1),(1,0),(1,1)] ] # i.e. just one contexts containing all the possible combinations of features\n",
    "        super().__init__(arms_bids, arms_prices, margin_param, contexts, M = M)\n",
    "        # self.features_partitioned = np.zeros(self.n_features)#notation 0 if that feature is not related to the partition, 1 if it is\n",
    "\n",
    "        self.window_context = window_context\n",
    "\n",
    "        # we need\n",
    "        self.collected_pulled_arm_bid = []# to save the idx of the pulled arms ordered by features not by context\n",
    "        self.collected_pulled_arm_price = []# to save the idx of the pulled arms ordered by features not by context\n",
    "        #COM: we need to collect all these informations because they are used when a new context structure is built\n",
    "\n",
    "\n",
    "    # def pull_arm(self): the same of the super class\n",
    "\n",
    "    def collect_pulled_arms(self, pulled_arm_bid_list, pulled_arm_price_list, features_list):\n",
    "        idx_bid_list = []#[[] for fts in features_list]\n",
    "        idx_price_list =  []#[[] for fts in features_list]\n",
    "\n",
    "        for fts in features_list:#i,fts in enumerate(features_list):\n",
    "            for j,context in enumerate(self.contexts):\n",
    "                if fts in context:\n",
    "                    # idx_bid_list[i] = pulled_arm_bid_list[j]\n",
    "                    # idx_price_list[i] = pulled_arm_price_list[j]\n",
    "                    idx_bid_list.append(pulled_arm_bid_list[j])\n",
    "                    idx_price_list.append(pulled_arm_price_list[j])\n",
    "                    break\n",
    "\n",
    "        self.collected_pulled_arm_bid.append(idx_bid_list)\n",
    "        self.collected_pulled_arm_price.append(idx_price_list)\n",
    "\n",
    "\n",
    "    def get_total_lb_from_contexts(self, context_learners):\n",
    "        lb_reward_tot = 0\n",
    "        for context_learner in context_learners:\n",
    "            lb_clicks = min(context_learner.n_daily_clicks_learner.means - context_learner.n_daily_clicks_learner.sigmas)\n",
    "            lb_convertion_rate = min(context_learner.conversion_rate_learner.empirical_means - context_learner.conversion_rate_learner.confidence)\n",
    "            ub_costs = max(context_learner.cum_daily_costs_learner.means + context_learner.cum_daily_costs_learner.sigmas)\n",
    "\n",
    "            lb_reward = lb_clicks * lb_convertion_rate * self.margin - ub_costs\n",
    "            lb_reward_tot += lb_reward\n",
    "        return lb_reward_tot\n",
    "\n",
    "\n",
    "    def get_total_lb_from_contexts(self, context_learners):\n",
    "        lb_reward_tot = 0\n",
    "        for context_learner in context_learners:            \n",
    "            idx_bid_arm, idx_price_arm = context_learner.pull_arm()\n",
    "            \n",
    "            lb_clicks = context_learner.n_daily_clicks_learner.means[idx_bid_arm] - 1.65 * context_learner.n_daily_clicks_learner.sigmas[idx_bid_arm]\n",
    "            \n",
    "            # convertion_rate_hat = ...\n",
    "            dim_setdata = len(context_learner.expected_margin_learner.rewards_per_arm[idx_price_arm])\n",
    "            expected_margin_hat = context_learner.expected_margin_learner.empirical_means[idx_price_arm]\n",
    "            lb_convertion_rate = expected_margin_hat - np.sqrt(-np.log(0.95)/(2 * dim_setdata))\n",
    "            \n",
    "            ub_costs = context_learner.cum_daily_costs_learner.means[idx_bid_arm] + 1.65 * context_learner.cum_daily_costs_learner.sigmas[idx_bid_arm]\n",
    "            \n",
    "\n",
    "            lb_reward = lb_clicks * lb_convertion_rate * (self.arms_prices[idx_price_arm] - self.margin_param) - ub_costs\n",
    "            lb_reward_tot += lb_reward\n",
    "        return lb_reward_tot\n",
    "\n",
    "\n",
    "    def generate_partitions(self):\n",
    "        \"\"\"Greedy approach:\n",
    "        returns a list of the partitions to be evaluated\n",
    "        \"\"\"\n",
    "        ## General approach if n_features = any number ??\n",
    "        # something that uses self.features_partitioned and self.contexts to generate the partitions\n",
    "        # and return the partitions and the indicators of which features are used for those partitions\n",
    "        #\n",
    "        # since n_features = 2 it is just:\n",
    "        partitions_list = []\n",
    "        if len(self.contexts) == 1:\n",
    "            partitions_list = [ [[(0,0),(0,1)], [(1,0),(1,1)]], [[(0,0),(1,0)], [(0,1),(1,1)]] ]\n",
    "        if len(self.contexts) == 2:\n",
    "            partitions_list = [ [[(0,0)], [(0,1)], [(1,0)], [(1,1)]] ]\n",
    "        if len(self.contexts) == 4:\n",
    "            partitions_list = []\n",
    "\n",
    "        return partitions_list\n",
    "\n",
    "\n",
    "    def update(self, pulled_arm_bid_list, pulled_arm_price_list,\n",
    "               n_daily_clicks_list, cum_daily_costs_list, reward_list, features_list):\n",
    "        \"\"\"Obs: length of the lists of the pulled arms not necessary equal to those in the lists of clicks costs rewards and features\"\"\"\n",
    "\n",
    "        super().update(pulled_arm_bid_list, pulled_arm_price_list, n_daily_clicks_list, cum_daily_costs_list, reward_list, features_list)\n",
    "        self.collect_pulled_arms(pulled_arm_bid_list, pulled_arm_price_list, features_list)\n",
    "\n",
    "        if self.t % self.window_context == 0: #then consider to change the structure of the contexts\n",
    "            print(\"evaluating change at time\", self.t)\n",
    "            lb_reward_current_partition = self.get_total_lb_from_contexts(self.context_learners)\n",
    "            partitions_to_evaluate = self.generate_partitions()\n",
    "\n",
    "            for partition in partitions_to_evaluate:\n",
    "                new_context_learners = [UCB_SingleContextLearner(self.arms_bids, self.arms_prices, self.margin, context, self.M) for context in partition]\n",
    "\n",
    "                for i,context in enumerate(partition):\n",
    "                    # finding the data to fit the context learner:\n",
    "                    pulled_arm_bid_data = []\n",
    "                    pulled_arm_price_data = []\n",
    "                    n_daily_clicks_data = []\n",
    "                    cum_daily_costs_data = []\n",
    "                    reward_data = []\n",
    "                    for t, collected_features_t in enumerate(self.collected_features):\n",
    "                        for j, fts in enumerate(collected_features_t):\n",
    "                            if fts in context:\n",
    "                                pulled_arm_bid_data.append(self.collected_pulled_arm_bid[t][j])\n",
    "                                pulled_arm_price_data.append(self.collected_pulled_arm_price[t][j])\n",
    "\n",
    "                                n_daily_clicks_data.append(self.collected_n_daily_clicks[t][j])\n",
    "                                cum_daily_costs_data.append(self.collected_cum_daily_costs[t][j])\n",
    "                                reward_data.append(self.collected_rewards[t][j])\n",
    "\n",
    "                    # fitting the context learner\n",
    "                    new_context_learners[i].update_from_batch(pulled_arm_bid_data, pulled_arm_price_data,\n",
    "                                                              n_daily_clicks_data, cum_daily_costs_data, reward_data, self.t)\n",
    "                                                              \n",
    "                ## evaluate lower bound of the new partition\n",
    "                lb_reward_new_partition = self.get_total_lb_from_contexts(new_context_learners)\n",
    "                print(\"  current lb reward = \", lb_reward_current_partition)\n",
    "                print(\"  candidate partition lb reward = \", lb_reward_new_partition)\n",
    "\n",
    "                # Now we need to compare this lb with the lb of the current partition to choose which is better\n",
    "                if lb_reward_new_partition > lb_reward_current_partition:\n",
    "                    self.contexts = partition.copy()\n",
    "                    self.context_learners = new_context_learners\n",
    "                    lb_reward_current_partition = lb_reward_new_partition\n",
    "\n",
    "                    print(f'PARTITION CHANGED AT TIME {self.t}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Com: It is like of the Custom_S3_TS_Learner with the following modifications:\n",
    "\n",
    "Modifications:\n",
    "- new attribute: context (actually, not really necesary)\n",
    "- new method: update_from_batch\n",
    "\"\"\"\n",
    "\n",
    "class TS_SingleContextLearner(Learner):\n",
    "    def __init__(self, arms_bids, arms_prices, margin_param, context):\n",
    "        n_arms_bids = len(arms_bids)\n",
    "        n_arms_prices = len(arms_prices)\n",
    "        \n",
    "        super().__init__(0)\n",
    "        self.rewards_per_arm = [[[] for i in range(n_arms_prices)] for j in range(n_arms_bids)]\n",
    "\n",
    "        self.arms_prices = arms_prices\n",
    "        self.arms_bids = arms_bids\n",
    "        \n",
    "        self.margin_param = margin_param\n",
    "\n",
    "        #learners\n",
    "        self.n_daily_clicks_learner = GPTS_Learner(arms_bids)\n",
    "        self.cum_daily_costs_learner = GPTS_Learner(arms_bids)\n",
    "        self.conversion_rate_learner = Binomial_TS_Learner(n_arms_prices)\n",
    "\n",
    "        self.context = context# it is a list of tuples #\n",
    "        \n",
    "\n",
    "    def pull_arm(self):\n",
    "        sampled_conversion_rates = np.random.beta( self.conversion_rate_learner.beta_parameters[:,0], self.conversion_rate_learner.beta_parameters[:,1] )\n",
    "        sampled_clicks = np.random.normal(self.n_daily_clicks_learner.means, self.n_daily_clicks_learner.sigmas)\n",
    "        sampled_costs = np.random.normal(self.cum_daily_costs_learner.means, self.cum_daily_costs_learner.sigmas)\n",
    "        \n",
    "        idx_price_arm = np.argmax(sampled_conversion_rates * (self.arms_prices - self.margin_param))\n",
    "\n",
    "        idx_bid_arm = np.argmax(\n",
    "            sampled_clicks * sampled_conversion_rates[idx_price_arm] * (self.arms_prices[idx_price_arm] - self.margin_param) - sampled_costs\n",
    "            )\n",
    "        \n",
    "        return idx_bid_arm, idx_price_arm\n",
    "    \n",
    "    \n",
    "    def update_observations(self, pulled_arm_bid, pulled_arm_price, reward):\n",
    "        self.rewards_per_arm[pulled_arm_bid][pulled_arm_price].append(reward)\n",
    "        self.collected_rewards.append(reward)\n",
    "        \n",
    "\n",
    "    def update(self, pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward):\n",
    "        \"\"\"\"\n",
    "        Obs: if n_daily_clicks = 0, we do not update the conversion_rate_learner\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        self.update_observations(pulled_arm_bid, pulled_arm_price, reward)\n",
    "\n",
    "        self.n_daily_clicks_learner.update(pulled_arm_bid, n_daily_clicks)\n",
    "        self.cum_daily_costs_learner.update(pulled_arm_bid, cum_daily_costs)\n",
    "\n",
    "        if n_daily_clicks > 0:\n",
    "            self.conversion_rate_learner.update(pulled_arm_price, converted_clicks, n_daily_clicks)\n",
    "        else:\n",
    "            self.conversion_rate_learner.t += 1# need to be increased, because t is used in the computation of the upperbound\n",
    "            \n",
    "\n",
    "    def update_from_batch(self, pulled_arm_bid_data, pulled_arm_price_data, n_daily_clicks_data, cum_daily_costs_data, converted_clicks_data, reward_data, time):\n",
    "        \"\"\"Obs:\n",
    "            - not all rewards are valid to update the convertion rate, because n_clicks can be 0 in some of values in n_daily_clicks_data\n",
    "        \"\"\"\n",
    "        arms_data = []\n",
    "        k_data = []\n",
    "        k_max_data = []\n",
    "        for i, reward in enumerate(reward_data):\n",
    "            self.update_observations(pulled_arm_bid_data[i], pulled_arm_price_data[i], reward)\n",
    "            if n_daily_clicks_data[i] > 0:\n",
    "                arms_data.append(pulled_arm_price_data[i])\n",
    "                k_data.append(converted_clicks_data[i])\n",
    "                k_max_data.append(n_daily_clicks_data[i])\n",
    "\n",
    "        self.conversion_rate_learner.update_from_batch(arms_data, k_data, k_max_data, time)\n",
    "        self.n_daily_clicks_learner.update_from_batch(pulled_arm_bid_data, n_daily_clicks_data, time)\n",
    "        self.cum_daily_costs_learner.update_from_batch(pulled_arm_bid_data, cum_daily_costs_data, time)\n",
    "        self.t += time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS_FixedContextsLearner(Learner):\n",
    "    def __init__(self, arms_bids, arms_prices, margin_param, contexts):\n",
    "        super().__init__(0)# This learner learns by context and not by arms directly (the arms are inside the context_learners).\n",
    "\n",
    "        self.collected_n_daily_clicks = []\n",
    "        self.collected_cum_daily_costs = []\n",
    "        self.collected_converted_clicks = []\n",
    "        self.collected_features = []\n",
    "\n",
    "        self.n_arms_bids = len(arms_bids)\n",
    "        self.n_arms_prices = len(arms_prices)\n",
    "        self.arms_prices = arms_prices\n",
    "        self.arms_bids = arms_bids\n",
    "\n",
    "        self.margin_param = margin_param\n",
    "        self.contexts = contexts\n",
    "\n",
    "        # Learners:\n",
    "        self.context_learners = [TS_SingleContextLearner(arms_bids, arms_prices, margin_param, context) for context in self.contexts]\n",
    "\n",
    "\n",
    "    def pull_arm(self):\n",
    "        idx_bid_arm_list = []\n",
    "        idx_price_arm_list = []\n",
    "        for context_learner in self.context_learners:\n",
    "            idx_bid_arm, idx_price_arm = context_learner.pull_arm()\n",
    "            idx_bid_arm_list.append(idx_bid_arm)\n",
    "            idx_price_arm_list.append(idx_price_arm)\n",
    "\n",
    "        return idx_bid_arm_list, idx_price_arm_list, self.contexts\n",
    "\n",
    "\n",
    "    def update_observations(self, n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list):\n",
    "        # no self.rewards_per_arm[pulled_arm].append(reward) because no concept of an arm (the arms are used inside the context learners)\n",
    "        self.collected_rewards.append(reward_list)\n",
    "        self.collected_n_daily_clicks.append(n_daily_clicks_list)\n",
    "        self.collected_cum_daily_costs.append(cum_daily_costs_list)\n",
    "        self.collected_converted_clicks.append(converted_clicks_list)\n",
    "        self.collected_features.append(features_list)\n",
    "        \n",
    "\n",
    "    def update(self, pulled_arm_bid_list, pulled_arm_price_list,\n",
    "               n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list):\n",
    "        \"\"\"Obs: the length of the lists of the pulled arms are not necessary equal to those in the lists of clicks, costs, rewards and features\"\"\"\n",
    "        self.t += 1\n",
    "        self.update_observations(n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list)\n",
    "\n",
    "        for i,context in enumerate(self.contexts):\n",
    "            pulled_arm_bid_data = []\n",
    "            pulled_arm_price_data = []\n",
    "\n",
    "            n_daily_clicks_data = []\n",
    "            cum_daily_costs_data = []\n",
    "            converted_clicks_data = []\n",
    "            reward_data = []\n",
    "            for j,fts in enumerate(features_list):\n",
    "                if fts in context:\n",
    "                    pulled_arm_bid_data.append(pulled_arm_bid_list[i])\n",
    "                    pulled_arm_price_data.append(pulled_arm_price_list[i])\n",
    "\n",
    "                    n_daily_clicks_data.append(n_daily_clicks_list[j])\n",
    "                    cum_daily_costs_data.append(cum_daily_costs_list[j])\n",
    "                    converted_clicks_data.append(converted_clicks_list[j])\n",
    "                    reward_data.append(reward_list[j])\n",
    "\n",
    "            self.context_learners[i].update_from_batch(pulled_arm_bid_data, pulled_arm_price_data,\n",
    "                                                       n_daily_clicks_data, cum_daily_costs_data, converted_clicks_data, reward_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS_ContextsGreedyLearner(TS_FixedContextsLearner):\n",
    "    def __init__(self, arms_bids, arms_prices, margin_param, window_context):\n",
    "        # self.n_features = 2\n",
    "        contexts = [ [(0,0),(0,1),(1,0),(1,1)] ] # i.e. just one contexts containing all the possible combinations of features\n",
    "        super().__init__(arms_bids, arms_prices, margin_param, contexts)\n",
    "        # self.features_partitioned = np.zeros(self.n_features)#notation 0 if that feature is not related to the partition, 1 if it is\n",
    "\n",
    "        self.window_context = window_context\n",
    "\n",
    "        # we need\n",
    "        self.collected_pulled_arm_bid = []# to save the idx of the pulled arms ordered by features not by context\n",
    "        self.collected_pulled_arm_price = []# to save the idx of the pulled arms ordered by features not by context\n",
    "        #COM: we need to collect all these informations because they are used when a new context structure is built\n",
    "\n",
    "\n",
    "    # def pull_arm(self): the same of the super class\n",
    "\n",
    "    def collect_pulled_arms(self, pulled_arm_bid_list, pulled_arm_price_list, features_list):\n",
    "        idx_bid_list = []#[[] for fts in features_list]\n",
    "        idx_price_list =  []#[[] for fts in features_list]\n",
    "\n",
    "        for fts in features_list:#i,fts in enumerate(features_list):\n",
    "            for j,context in enumerate(self.contexts):\n",
    "                if fts in context:\n",
    "                    # idx_bid_list[i] = pulled_arm_bid_list[j]\n",
    "                    # idx_price_list[i] = pulled_arm_price_list[j]\n",
    "                    idx_bid_list.append(pulled_arm_bid_list[j])\n",
    "                    idx_price_list.append(pulled_arm_price_list[j])\n",
    "                    break\n",
    "\n",
    "        self.collected_pulled_arm_bid.append(idx_bid_list)\n",
    "        self.collected_pulled_arm_price.append(idx_price_list)\n",
    "\n",
    "\n",
    "    def get_total_lb_from_contexts(self, context_learners):\n",
    "        lb_reward_tot = 0\n",
    "        for context_learner in context_learners:\n",
    "            idx_bid_arm, idx_price_arm = context_learner.pull_arm()\n",
    "            \n",
    "            lb_clicks = context_learner.n_daily_clicks_learner.means[idx_bid_arm] - 1.65 * context_learner.n_daily_clicks_learner.sigmas[idx_bid_arm]\n",
    "            \n",
    "            dim_setdata_cr = (context_learner.conversion_rate_learner.beta_parameters[idx_price_arm,0] + context_learner.conversion_rate_learner.beta_parameters[idx_price_arm,1])\n",
    "            convertion_rate_hat = context_learner.conversion_rate_learner.beta_parameters[idx_price_arm,0] / dim_setdata_cr\n",
    "            lb_convertion_rate = convertion_rate_hat - np.sqrt(-np.log(0.95)/(2 * dim_setdata_cr))\n",
    "            \n",
    "            ub_costs = context_learner.cum_daily_costs_learner.means[idx_bid_arm] + 1.65 * context_learner.cum_daily_costs_learner.sigmas[idx_bid_arm]\n",
    "\n",
    "            lb_reward = lb_clicks * lb_convertion_rate * (self.arms_prices[idx_price_arm] - self.margin_param) - ub_costs\n",
    "            lb_reward_tot += lb_reward\n",
    "        return lb_reward_tot\n",
    "\n",
    "\n",
    "    def generate_partitions(self):\n",
    "        \"\"\"Greedy approach:\n",
    "        returns a list of the partitions to be evaluated\n",
    "        \"\"\"\n",
    "        ## General approach if n_features = any number ??\n",
    "        # something that uses self.features_partitioned and self.contexts to generate the partitions\n",
    "        # and return the partitions and the indicators of which features are used for those partitions\n",
    "        #\n",
    "        # since n_features = 2 it is just:\n",
    "        partitions_list = []\n",
    "        if len(self.contexts) == 1:\n",
    "            partitions_list = [ [[(0,0),(0,1)], [(1,0),(1,1)]], [[(0,0),(1,0)], [(0,1),(1,1)]] ]\n",
    "        if len(self.contexts) == 2:\n",
    "            partitions_list = [ [[(0,0)], [(0,1)], [(1,0)], [(1,1)]] ]\n",
    "        if len(self.contexts) == 4:\n",
    "            partitions_list = []\n",
    "\n",
    "        return partitions_list\n",
    "\n",
    "\n",
    "    def update(self, pulled_arm_bid_list, pulled_arm_price_list,\n",
    "               n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list):\n",
    "        \"\"\"Obs: length of the lists of the pulled arms not necessary equal to those in the lists of clicks costs rewards and features\"\"\"\n",
    "\n",
    "        super().update(pulled_arm_bid_list, pulled_arm_price_list, n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list)\n",
    "        self.collect_pulled_arms(pulled_arm_bid_list, pulled_arm_price_list, features_list)\n",
    "\n",
    "        if self.t % self.window_context == 0: #then consider to change the structure of the contexts\n",
    "            print(\"evaluating change at time\", self.t)\n",
    "            lb_reward_current_partition = self.get_total_lb_from_contexts(self.context_learners)\n",
    "            partitions_to_evaluate = self.generate_partitions()\n",
    "\n",
    "            for partition in partitions_to_evaluate:\n",
    "                new_context_learners = [TS_SingleContextLearner(self.arms_bids, self.arms_prices, self.margin_param, context) for context in partition]\n",
    "\n",
    "                for i,context in enumerate(partition):\n",
    "                    # finding the data to fit the context learner:\n",
    "                    pulled_arm_bid_data = []\n",
    "                    pulled_arm_price_data = []\n",
    "                    n_daily_clicks_data = []\n",
    "                    cum_daily_costs_data = []\n",
    "                    converted_clicks_data = []\n",
    "                    reward_data = []\n",
    "                    for t, collected_features_t in enumerate(self.collected_features):\n",
    "                        for j, fts in enumerate(collected_features_t):\n",
    "                            if fts in context:\n",
    "                                pulled_arm_bid_data.append(self.collected_pulled_arm_bid[t][j])\n",
    "                                pulled_arm_price_data.append(self.collected_pulled_arm_price[t][j])\n",
    "\n",
    "                                n_daily_clicks_data.append(self.collected_n_daily_clicks[t][j])\n",
    "                                cum_daily_costs_data.append(self.collected_cum_daily_costs[t][j])\n",
    "                                converted_clicks_data.append(self.collected_converted_clicks[t][j])\n",
    "                                reward_data.append(self.collected_rewards[t][j])\n",
    "\n",
    "                    # fitting the context learner\n",
    "                    new_context_learners[i].update_from_batch(pulled_arm_bid_data, pulled_arm_price_data,\n",
    "                                                              n_daily_clicks_data, cum_daily_costs_data, converted_clicks_data, reward_data, self.t)\n",
    "\n",
    "                ## evaluate lower bound of the new partition\n",
    "                lb_reward_new_partition = self.get_total_lb_from_contexts(new_context_learners)\n",
    "                print(\"  current lb reward = \", lb_reward_current_partition)\n",
    "                print(\"  candidate partition lb reward = \", lb_reward_new_partition)\n",
    "\n",
    "                # Now we need to compare this lb with the lb of the current partition to choose which is better\n",
    "                if lb_reward_new_partition > lb_reward_current_partition:\n",
    "                    self.contexts = partition.copy()\n",
    "                    self.context_learners = new_context_learners\n",
    "                    lb_reward_current_partition = lb_reward_new_partition\n",
    "\n",
    "                    print(f'PARTITION CHANGED AT TIME {self.t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 365\n",
    "n_experiments = 20 #50\n",
    "\n",
    "# context parameters:\n",
    "window_context = 14\n",
    "\n",
    "\n",
    "# param ucb\n",
    "M = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clairvoyant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_list = []\n",
    "\n",
    "for i, userclass in enumerate(user_classes):\n",
    "    opt_arm_price = np.argmax(userclass.conversion_rate_function(PRICES) * (PRICES - MARGIN_PARAM))\n",
    "    opt_price = PRICES[opt_arm_price]\n",
    "    opt_conversion_rate = userclass.conversion_rate_function(opt_price)\n",
    "\n",
    "\n",
    "    opt_bid_arm = np.argmax(userclass.n_daily_clicks_function(BIDS) * opt_conversion_rate * (opt_price - MARGIN_PARAM) - userclass.cum_daily_costs_function(BIDS))\n",
    "    opt_bid = BIDS[opt_bid_arm]\n",
    "    opt_n_daily_clicks = userclass.n_daily_clicks_function(opt_bid)\n",
    "    opt_cum_daily_costs = userclass.cum_daily_costs_function(opt_bid)\n",
    "\n",
    "\n",
    "    opt = opt_n_daily_clicks * opt_conversion_rate * (opt_price - MARGIN_PARAM) - opt_cum_daily_costs\n",
    "\n",
    "    opt_list.append(opt)\n",
    "\n",
    "    print(f'User class C{i}, context {real_contexts[i]}')\n",
    "    print(\"- optimal bid: \", opt_bid)\n",
    "    print(\"- optimal price: \", opt_price)\n",
    "    print()\n",
    "    print(\"- n_daily_clicks on the optimal bid: \", opt_n_daily_clicks)\n",
    "    print(\"- cum_daily_costs on the optimal bid: \", opt_cum_daily_costs)\n",
    "    print(\"- convertion rate on the optimal price: \", opt_conversion_rate)\n",
    "    print()\n",
    "    print(\"- optimal reward: \", opt)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: known structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB - known context structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UCB learner\n",
    "\n",
    "ucb_rewards_per_experiment = []\n",
    "\n",
    "for e in range(0, n_experiments):\n",
    "    print(f'{e/n_experiments*100}%')\n",
    "\n",
    "    env = Environment(BIDS, PRICES, SIGMA_CLICKS, SIGMA_COSTS, MARGIN_PARAM, user_classes)\n",
    "    ucb_learner = UCB_FixedContextsLearner(BIDS, PRICES, MARGIN_PARAM, real_contexts, M)\n",
    "\n",
    "    for t in range(0, T):\n",
    "        pulled_arm_bid, pulled_arm_price, pulled_contexts = ucb_learner.pull_arm()\n",
    "        n_daily_clicks, cum_daily_costs, converted_clicks, reward, features = env.round(pulled_arm_bid, pulled_arm_price, pulled_contexts)\n",
    "        ucb_learner.update(pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward, features)\n",
    "\n",
    "    ucb_rewards_per_experiment.append(ucb_learner.collected_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_simulations = np.array(ucb_rewards_per_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    std_inst = np.std(rewards_simulations[:,:,i], axis = 0)/np.sqrt(n_experiments)\n",
    "    std_cum = np.std(np.cumsum(rewards_simulations[:,:,i], axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "    mean_rewards_hat = np.mean(rewards_simulations[:,:,i], axis = 0)\n",
    "    opt = opt_list[i]\n",
    "\n",
    "    #Plots\n",
    "    plt.figure(figsize = (14,6))\n",
    "    plt.suptitle(f'Plots of user class C{i+1}')\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous reward\")\n",
    "    plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "    plt.plot(mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous regret\")\n",
    "    plt.plot(opt - mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative reward\")\n",
    "    plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "    plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative regret\")\n",
    "    plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rewards_simulations = np.sum(rewards_simulations, axis = 2)\n",
    "\n",
    "std_inst = np.std(sum_rewards_simulations, axis = 0)/np.sqrt(n_experiments)\n",
    "std_cum = np.std(np.cumsum(sum_rewards_simulations, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "mean_rewards_hat = np.mean(sum_rewards_simulations, axis = 0)\n",
    "opt = sum(opt_list)\n",
    "\n",
    "\n",
    "#Plots\n",
    "plt.figure(figsize = (14,6))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "plt.plot(mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TS - known context structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_experiments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TS learner\n",
    "\n",
    "ts_rewards_per_experiment = []\n",
    "\n",
    "for e in range(0, n_experiments):\n",
    "    print(f'{e/n_experiments*100}%')\n",
    "\n",
    "    env = Environment(BIDS, PRICES, SIGMA_CLICKS, SIGMA_COSTS, MARGIN_PARAM, user_classes)\n",
    "    ts_learner = TS_FixedContextsLearner(BIDS, PRICES, MARGIN_PARAM, real_contexts)\n",
    "\n",
    "    for t in range(0, T):\n",
    "        pulled_arm_bid, pulled_arm_price, pulled_contexts = ts_learner.pull_arm()\n",
    "        n_daily_clicks, cum_daily_costs, converted_clicks, reward, features = env.round(pulled_arm_bid, pulled_arm_price, pulled_contexts)\n",
    "        ts_learner.update(pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward, features)\n",
    "\n",
    "    ts_rewards_per_experiment.append(ts_learner.collected_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_simulations = np.array(ts_rewards_per_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    std_inst = np.std(rewards_simulations[:,:,i], axis = 0)/np.sqrt(n_experiments)\n",
    "    std_cum = np.std(np.cumsum(rewards_simulations[:,:,i], axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "    mean_rewards_hat = np.mean(rewards_simulations[:,:,i], axis = 0)\n",
    "    opt = opt_list[i]\n",
    "\n",
    "    #Plots\n",
    "    plt.figure(figsize = (14,6))\n",
    "    plt.suptitle(f'Plots of user class C{i+1}')\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous reward\")\n",
    "    plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "    plt.plot(mean_rewards_hat, 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous regret\")\n",
    "    plt.plot(opt - mean_rewards_hat, 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative reward\")\n",
    "    plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "    plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative regret\")\n",
    "    plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rewards_simulations = np.sum(rewards_simulations, axis = 2)\n",
    "\n",
    "std_inst = np.std(sum_rewards_simulations, axis = 0)/np.sqrt(n_experiments)\n",
    "std_cum = np.std(np.cumsum(sum_rewards_simulations, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "mean_rewards_hat = np.mean(sum_rewards_simulations, axis = 0)\n",
    "opt = sum(opt_list)\n",
    "\n",
    "\n",
    "#Plots\n",
    "plt.figure(figsize = (14,6))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "plt.plot(mean_rewards_hat, 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - mean_rewards_hat, 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot both UCB and TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UCB\n",
    "ucb_std_inst = np.std(ucb_rewards_per_experiment, axis = 0) / np.sqrt(n_experiments)\n",
    "ucb_std_cum = np.std(np.cumsum(ucb_rewards_per_experiment, axis = 1), axis = 0) / np.sqrt(n_experiments)\n",
    "\n",
    "ucb_mean_rewards_hat = np.mean(ucb_rewards_per_experiment, axis = 0)\n",
    "\n",
    "## TS\n",
    "ts_std_inst = np.std(ts_rewards_per_experiment, axis = 0)/np.sqrt(n_experiments)\n",
    "ts_std_cum = np.std(np.cumsum(ts_rewards_per_experiment, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "ts_mean_rewards_hat = np.mean(ts_rewards_per_experiment, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting plots\n",
    "plt.figure(figsize = (15,7))\n",
    "\n",
    "#Comparing instantaneous reward\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"opt\")\n",
    "plt.plot(ucb_mean_rewards_hat, color = 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), ucb_mean_rewards_hat - ucb_std_inst, ucb_mean_rewards_hat + ucb_std_inst, alpha = 0.2, color = 'C1')\n",
    "plt.plot(ts_mean_rewards_hat, color = 'C2', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), ts_mean_rewards_hat-ts_std_inst, ts_mean_rewards_hat + ts_std_inst, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing instantaneous regret\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - ucb_mean_rewards_hat, color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), opt - ucb_mean_rewards_hat - ucb_std_inst, opt - ucb_mean_rewards_hat + ucb_std_inst, alpha = 0.2, color = 'C1')\n",
    "plt.plot(opt - ts_mean_rewards_hat, color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), opt - ts_mean_rewards_hat - ts_std_inst, opt - ts_mean_rewards_hat + ts_std_inst, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing cumulative reward\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(ucb_mean_rewards_hat), color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(ucb_mean_rewards_hat) - ucb_std_cum, np.cumsum(ucb_mean_rewards_hat) + ucb_std_cum, alpha = 0.2, color = 'C1')\n",
    "plt.plot(np.cumsum(ts_mean_rewards_hat), color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(ts_mean_rewards_hat) - ts_std_cum, np.cumsum(ts_mean_rewards_hat) + ts_std_cum, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing cumulative regret\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - ucb_mean_rewards_hat), color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(opt - ucb_mean_rewards_hat) - ucb_std_cum, np.cumsum(opt - ucb_mean_rewards_hat) + ucb_std_cum, alpha = 0.2, color = 'C1')\n",
    "plt.plot(np.cumsum(opt - ts_mean_rewards_hat), color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(opt - ts_mean_rewards_hat) - ts_std_cum, np.cumsum(opt - ts_mean_rewards_hat) + ts_std_cum, alpha = 0.2, color = 'C2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: unknown structure & no context generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB - known context structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UCB learner\n",
    "\n",
    "ucb_rewards_per_experiment = []\n",
    "\n",
    "for e in range(0, n_experiments):\n",
    "    print(f'{e/n_experiments*100}%')\n",
    "\n",
    "    env = Environment(BIDS, PRICES, SIGMA_CLICKS, SIGMA_COSTS, MARGIN_PARAM, user_classes)\n",
    "    ucb_learner = UCB_FixedContextsLearner(BIDS, PRICES, MARGIN_PARAM, [ [(0,0),(0,1),(1,0),(1,1)] ])\n",
    "\n",
    "    for t in range(0, T):\n",
    "        pulled_arm_bid, pulled_arm_price, pulled_contexts = ucb_learner.pull_arm()\n",
    "        n_daily_clicks, cum_daily_costs, converted_clicks, reward, features = env.round(pulled_arm_bid, pulled_arm_price, pulled_contexts)\n",
    "        ucb_learner.update(pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward, features)\n",
    "\n",
    "    ucb_rewards_per_experiment.append(ucb_learner.collected_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_simulations = np.array(ucb_rewards_per_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    std_inst = np.std(rewards_simulations[:,:,i], axis = 0)/np.sqrt(n_experiments)\n",
    "    std_cum = np.std(np.cumsum(rewards_simulations[:,:,i], axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "    mean_rewards_hat = np.mean(rewards_simulations[:,:,i], axis = 0)\n",
    "    opt = opt_list[i]\n",
    "\n",
    "    #Plots\n",
    "    plt.figure(figsize = (14,6))\n",
    "    plt.suptitle(f'Plots of user class C{i+1}')\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous reward\")\n",
    "    plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "    plt.plot(mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous regret\")\n",
    "    plt.plot(opt - mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative reward\")\n",
    "    plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "    plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative regret\")\n",
    "    plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rewards_simulations = np.sum(rewards_simulations, axis = 2)\n",
    "\n",
    "std_inst = np.std(sum_rewards_simulations, axis = 0)/np.sqrt(n_experiments)\n",
    "std_cum = np.std(np.cumsum(sum_rewards_simulations, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "mean_rewards_hat = np.mean(sum_rewards_simulations, axis = 0)\n",
    "opt = sum(opt_list)\n",
    "\n",
    "\n",
    "#Plots\n",
    "plt.figure(figsize = (14,6))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "plt.plot(mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TS - known context structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_experiments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TS learner\n",
    "\n",
    "ts_rewards_per_experiment = []\n",
    "\n",
    "for e in range(0, n_experiments):\n",
    "    print(f'{e/n_experiments*100}%')\n",
    "\n",
    "    env = Environment(BIDS, PRICES, SIGMA_CLICKS, SIGMA_COSTS, MARGIN_PARAM, user_classes)\n",
    "    ts_learner = TS_FixedContextsLearner(BIDS, PRICES, MARGIN_PARAM, real_contexts)\n",
    "\n",
    "    for t in range(0, T):\n",
    "        pulled_arm_bid, pulled_arm_price, pulled_contexts = ts_learner.pull_arm()\n",
    "        n_daily_clicks, cum_daily_costs, converted_clicks, reward, features = env.round(pulled_arm_bid, pulled_arm_price, pulled_contexts)\n",
    "        ts_learner.update(pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward, features)\n",
    "\n",
    "    ts_rewards_per_experiment.append(ts_learner.collected_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_simulations = np.array(ts_rewards_per_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    std_inst = np.std(rewards_simulations[:,:,i], axis = 0)/np.sqrt(n_experiments)\n",
    "    std_cum = np.std(np.cumsum(rewards_simulations[:,:,i], axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "    mean_rewards_hat = np.mean(rewards_simulations[:,:,i], axis = 0)\n",
    "    opt = opt_list[i]\n",
    "\n",
    "    #Plots\n",
    "    plt.figure(figsize = (14,6))\n",
    "    plt.suptitle(f'Plots of user class C{i+1}')\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous reward\")\n",
    "    plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "    plt.plot(mean_rewards_hat, 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous regret\")\n",
    "    plt.plot(opt - mean_rewards_hat, 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative reward\")\n",
    "    plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "    plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative regret\")\n",
    "    plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rewards_simulations = np.sum(rewards_simulations, axis = 2)\n",
    "\n",
    "std_inst = np.std(sum_rewards_simulations, axis = 0)/np.sqrt(n_experiments)\n",
    "std_cum = np.std(np.cumsum(sum_rewards_simulations, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "mean_rewards_hat = np.mean(sum_rewards_simulations, axis = 0)\n",
    "opt = sum(opt_list)\n",
    "\n",
    "\n",
    "#Plots\n",
    "plt.figure(figsize = (14,6))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "plt.plot(mean_rewards_hat, 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - mean_rewards_hat, 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot both UCB and TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UCB\n",
    "ucb_std_inst = np.std(ucb_rewards_per_experiment, axis = 0) / np.sqrt(n_experiments)\n",
    "ucb_std_cum = np.std(np.cumsum(ucb_rewards_per_experiment, axis = 1), axis = 0) / np.sqrt(n_experiments)\n",
    "\n",
    "ucb_mean_rewards_hat = np.mean(ucb_rewards_per_experiment, axis = 0)\n",
    "\n",
    "## TS\n",
    "ts_std_inst = np.std(ts_rewards_per_experiment, axis = 0)/np.sqrt(n_experiments)\n",
    "ts_std_cum = np.std(np.cumsum(ts_rewards_per_experiment, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "ts_mean_rewards_hat = np.mean(ts_rewards_per_experiment, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting plots\n",
    "plt.figure(figsize = (15,7))\n",
    "\n",
    "#Comparing instantaneous reward\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"opt\")\n",
    "plt.plot(ucb_mean_rewards_hat, color = 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), ucb_mean_rewards_hat - ucb_std_inst, ucb_mean_rewards_hat + ucb_std_inst, alpha = 0.2, color = 'C1')\n",
    "plt.plot(ts_mean_rewards_hat, color = 'C2', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), ts_mean_rewards_hat-ts_std_inst, ts_mean_rewards_hat + ts_std_inst, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing instantaneous regret\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - ucb_mean_rewards_hat, color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), opt - ucb_mean_rewards_hat - ucb_std_inst, opt - ucb_mean_rewards_hat + ucb_std_inst, alpha = 0.2, color = 'C1')\n",
    "plt.plot(opt - ts_mean_rewards_hat, color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), opt - ts_mean_rewards_hat - ts_std_inst, opt - ts_mean_rewards_hat + ts_std_inst, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing cumulative reward\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(ucb_mean_rewards_hat), color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(ucb_mean_rewards_hat) - ucb_std_cum, np.cumsum(ucb_mean_rewards_hat) + ucb_std_cum, alpha = 0.2, color = 'C1')\n",
    "plt.plot(np.cumsum(ts_mean_rewards_hat), color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(ts_mean_rewards_hat) - ts_std_cum, np.cumsum(ts_mean_rewards_hat) + ts_std_cum, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing cumulative regret\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - ucb_mean_rewards_hat), color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(opt - ucb_mean_rewards_hat) - ucb_std_cum, np.cumsum(opt - ucb_mean_rewards_hat) + ucb_std_cum, alpha = 0.2, color = 'C1')\n",
    "plt.plot(np.cumsum(opt - ts_mean_rewards_hat), color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(opt - ts_mean_rewards_hat) - ts_std_cum, np.cumsum(opt - ts_mean_rewards_hat) + ts_std_cum, alpha = 0.2, color = 'C2')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
