{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for GP\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, DotProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UserClass import *\n",
    "from basic_environments import *\n",
    "from basic_learners import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DATA_users.py\n",
    "%run DATA_parameters.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obs: it is the same environment of step3, with self.user_class commented\n",
    "class ContextEnvironment:\n",
    "    def __init__(self, bids, prices, sigma_clicks, sigma_costs, margin_param, userclass):\n",
    "        # self.userclass = userclass\n",
    "\n",
    "        self.bidding_environment = BiddingEnvironment(bids, sigma_clicks, sigma_costs, userclass)\n",
    "        self.pricing_environment = PricingEnvironment(prices, margin_param, userclass)\n",
    "\n",
    "\n",
    "    def round(self, pulled_arm_bid, pulled_arm_price):\n",
    "        n_daily_clicks, cum_daily_costs = self.bidding_environment.round(pulled_arm_bid)\n",
    "        converted_clicks, reward = self.pricing_environment.round(pulled_arm_price, n_daily_clicks, cum_daily_costs)\n",
    "\n",
    "        return n_daily_clicks, cum_daily_costs, converted_clicks, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, bids, prices, sigma_clicks, sigma_costs, margin_param, user_classes):\n",
    "        self.user_classes = user_classes\n",
    "        self.n_classes = len(user_classes)\n",
    "\n",
    "        self.single_environments = [ContextEnvironment(bids, prices, sigma_clicks, sigma_costs, margin_param, user_class) for user_class in user_classes]\n",
    "        self.features_list = [(user_class.F1,user_class.F2) for user_class in user_classes]\n",
    "\n",
    "    def round(self, pulled_arm_bid_list, pulled_arm_price_list, partition = [ [(0,0),(0,1),(1,0),(1,1)] ]):\n",
    "        \"\"\"\n",
    "        pulled_arm_bid_list: list of integers\n",
    "            idx of the pulled arm ob the bids for each component of the partition\n",
    "\n",
    "        pulled_arm_price_list: list of integers\n",
    "            idx of the pulled arm ob the prices for each component of the partition\n",
    "\n",
    "        partition: list of lists of tuples of integers\n",
    "            notation: each element must contain a tuple indicating the partitioning for the features F1 and F2\n",
    "            e.g.\n",
    "                if no partition ((the same as one set containing all possible features in the partition)): partition = [ [(0,0),(0,1),(1,0),(1,1)] ]\n",
    "                if partition by splitting F1: partition = [ [(0,0),(0,1)] , [(1,0),(1,1)] ]\n",
    "\n",
    "        Note: pulled_arm_bid_list, pulled_arm_price_list, partition must have the same length\n",
    "        \"\"\"\n",
    "\n",
    "        n_daily_clicks_list = []\n",
    "        cum_daily_costs_list = []\n",
    "        converted_clicks_list = []\n",
    "        reward_list = []\n",
    "        for i in range(self.n_classes):\n",
    "            user_F1 = self.user_classes[i].F1\n",
    "            user_F2 = self.user_classes[i].F2\n",
    "\n",
    "            idx_context = 0\n",
    "            for context in partition:\n",
    "                if (user_F1,user_F2) in context:\n",
    "                    break\n",
    "                idx_context += 1\n",
    "\n",
    "            n_daily_clicks, cum_daily_costs, converted_clicks, reward = self.single_environments[i].round(pulled_arm_bid_list[idx_context],\n",
    "                                                                                                          pulled_arm_price_list[idx_context])\n",
    "\n",
    "\n",
    "            n_daily_clicks_list.append(n_daily_clicks)\n",
    "            cum_daily_costs_list.append(cum_daily_costs)\n",
    "            converted_clicks_list.append(converted_clicks)\n",
    "            reward_list.append(reward)\n",
    "\n",
    "        return n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, self.features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Environment:\n",
    "#     def __init__(self, bids, prices, sigma_clicks, sigma_costs, margin_param, user_classes):\n",
    "#         self.user_classes = user_classes\n",
    "#         self.n_classes = len(user_classes)\n",
    "#         self.user_environments = [ContextEnvironment(bids, prices, sigma_clicks, sigma_costs, margin_param, user_class) for user_class in user_classes]\n",
    "#         self.features_list = [(user_class.F1,user_class.F2) for user_class in user_classes]\n",
    "        \n",
    "        \n",
    "#     def round(self, pulled_arm_bid_list, pulled_arm_price_list, pulled_contexts):\n",
    "#         \"\"\"\n",
    "#         pulled_arm_bid_list: list of integers\n",
    "#             idx of the pulled arm ob the bids for each component of the partition\n",
    "#         pulled_arm_price_list: list of integers\n",
    "#             idx of the pulled arm ob the prices for each component of the partition\n",
    "#         pulled_contexts: list of lists of tuples of integers\n",
    "#             notation: each element must contain a tuple indicating the partitioning for the features F1 and F2\n",
    "#             e.g.\n",
    "#                 if no partition ((the same as one set containing all possible features in the partition)): partition = [ [(0,0),(0,1),(1,0),(1,1)] ]\n",
    "#                 if partition by splitting F1: partition = [ [(0,0),(0,1)] , [(1,0),(1,1)] ]\n",
    "#         Note: pulled_arm_bid_list, pulled_arm_price_list, partition must have the same length\n",
    "#         About output: if pulled_contexts is valid (and is a partition), the length of each element in the output is the number of user classes \n",
    "#         \"\"\"\n",
    "        \n",
    "#         n_daily_clicks_list = []\n",
    "#         cum_daily_costs_list = []\n",
    "#         converted_clicks_list = []\n",
    "#         reward_list = []\n",
    "#         for user, user_env in zip(self.user_classes, self.user_environments):\n",
    "#             for pulled_arm_bid, pulled_arm_price, context in zip(pulled_arm_bid_list, pulled_arm_price_list, pulled_contexts):\n",
    "#                 n_daily_clicks, cum_daily_costs, converted_clicks, reward = user_env.round(pulled_arm_bid, pulled_arm_price)\n",
    "\n",
    "                \n",
    "#                 if (user.F1,user.F2) in context:\n",
    "#                     n_daily_clicks_list.append(n_daily_clicks)\n",
    "#                     cum_daily_costs_list.append(cum_daily_costs)\n",
    "#                     converted_clicks_list.append(converted_clicks)\n",
    "#                     reward_list.append(reward)\n",
    "#                     break\n",
    "#         return n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, self.features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learners\n",
    "\n",
    "hp:\n",
    "- we assume that the features we observe are binary  \n",
    "    e.g. if we have 2 features: (0,0), (0,1), (1,0), (1,1) are the all the possible observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Com: It is like of the Custom_S3_UCB_Learner with the following modifications:\n",
    "\n",
    "Modifications:\n",
    "- new attribute: context\n",
    "- new method: update_from_batch\n",
    "\"\"\"\n",
    "\n",
    "class UCB_SingleContextLearner(Learner):# Obs: the curve of n_clicks and cost are always the same for any price we put, and the conversion rate is always the same for any bid we put\n",
    "    def __init__(self, arms_bids, arms_prices, margin_param, context, M = 1):\n",
    "        n_arms_bids = len(arms_bids)\n",
    "        n_arms_prices = len(arms_prices)\n",
    "        super().__init__(0)\n",
    "        self.rewards_per_arm = [[[] for i in range(n_arms_prices)] for j in range(n_arms_bids)] # Notation: reward is in rewards_per_arm[idx_bid][idx_price]\n",
    "\n",
    "        self.arms_prices = arms_prices\n",
    "        self.arms_bids = arms_bids\n",
    "\n",
    "        self.margin_param = margin_param\n",
    "\n",
    "        #learners:\n",
    "        self.n_daily_clicks_learner = GPUCB_Learner(arms_bids)\n",
    "        self.cum_daily_costs_learner = GPUCB_Learner(arms_bids)\n",
    "        self.conversion_rate_learner = UCB1_Learner(n_arms_prices, (0,M))\n",
    "\n",
    "        self.context = context# it is a list of tuples #\n",
    "\n",
    "\n",
    "    def pull_arm(self):\n",
    "        \"\"\"\n",
    "        Notes:\n",
    "            The higher the conversion rate the higher the reward => I can pull directly the an arm of the prices for the highest alpha,\n",
    "            while the clicks and costs I have to evaluate them together and pull the bid that maximizes the reward\n",
    "\n",
    "            Question:\n",
    "                Once I know the price arm to be pulled, for the bid arm, should I consider the upperbound of the conversion rate or just the mean of the conversion rate?\n",
    "                -   choosing the mean of the conversion rate, there is just the uncertainty of related to the bid and the advertising curves\n",
    "                -   choosing the upperbound of the conversion rate, it adds uncertainty related to the problem \"conversion_rate(price)\"\n",
    "                --> I'm using the upperbound choice [but still an open question]\n",
    "        \"\"\"\n",
    "        ub_clicks = self.n_daily_clicks_learner.means + self.n_daily_clicks_learner.sigmas\n",
    "        lb_costs = self.cum_daily_costs_learner.means - self.cum_daily_costs_learner.sigmas\n",
    "        ub_conversion_rates = self.conversion_rate_learner.empirical_means + self.conversion_rate_learner.confidence\n",
    "\n",
    "        ub = ub_conversion_rates * (self.arms_prices - self.margin_param)\n",
    "        idx_price_arm = np.random.choice(np.where(ub == ub.max())[0])\n",
    "\n",
    "        ub_reward = ub_clicks * ub[idx_price_arm] - lb_costs\n",
    "        idx_bid_arm = np.random.choice(np.where(ub_reward == ub_reward.max())[0])\n",
    "\n",
    "        return (idx_bid_arm, idx_price_arm)\n",
    "    \n",
    "\n",
    "    def update_observations(self, pulled_arm_bid, pulled_arm_price, reward):\n",
    "        self.rewards_per_arm[pulled_arm_bid][pulled_arm_price].append(reward)\n",
    "        self.collected_rewards.append(reward)\n",
    "\n",
    "\n",
    "    def update(self, pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward):\n",
    "        \"\"\"\"\n",
    "        Obs: if n_daily_clicks = 0, we do not update the conversion_rate_learner\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        self.update_observations(pulled_arm_bid, pulled_arm_price, reward)\n",
    "\n",
    "        self.n_daily_clicks_learner.update(pulled_arm_bid, n_daily_clicks)\n",
    "        self.cum_daily_costs_learner.update(pulled_arm_bid, cum_daily_costs)\n",
    "\n",
    "        if n_daily_clicks > 0:\n",
    "            alpha = converted_clicks / n_daily_clicks\n",
    "            self.conversion_rate_learner.update(pulled_arm_price, alpha)\n",
    "        else:\n",
    "            self.conversion_rate_learner.t += 1# need to be increased, because t is used in the computation of the upperbound\n",
    "\n",
    "    \n",
    "    def update_from_batch(self, pulled_arm_bid_data, pulled_arm_price_data, n_daily_clicks_data, cum_daily_costs_data, converted_clicks_data, reward_data, time):######\n",
    "        \"\"\"Obs:\n",
    "            - not all rewards are valid to update the convertion rate, because n_clicks can be 0 in some of values in n_daily_clicks_data\n",
    "        \"\"\"\n",
    "        arms_of_alphas = []\n",
    "        alphas = []\n",
    "        for i, reward in enumerate(reward_data):\n",
    "            self.update_observations(pulled_arm_bid_data[i], pulled_arm_price_data[i], reward)\n",
    "            if n_daily_clicks_data[i] > 0:\n",
    "                arms_of_alphas.append(pulled_arm_price_data[i])\n",
    "                alphas.append(converted_clicks_data[i] / n_daily_clicks_data[i])\n",
    "\n",
    "        self.conversion_rate_learner.update_from_batch(arms_of_alphas, alphas, time)\n",
    "        self.n_daily_clicks_learner.update_from_batch(pulled_arm_bid_data, n_daily_clicks_data, time)\n",
    "        self.cum_daily_costs_learner.update_from_batch(pulled_arm_bid_data, cum_daily_costs_data, time)\n",
    "        self.t += time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB_FixedContextsLearner(Learner):\n",
    "    def __init__(self, arms_bids, arms_prices, margin_param, contexts):\n",
    "        super().__init__(0)# This learner learns by context and not by arms directly (the arms are inside the context_learners).\n",
    "\n",
    "        self.collected_n_daily_clicks = []\n",
    "        self.collected_cum_daily_costs = []\n",
    "        self.collected_converted_clicks = []\n",
    "        self.collected_features = []\n",
    "\n",
    "        self.n_arms_bids = len(arms_bids)\n",
    "        self.n_arms_prices = len(arms_prices)\n",
    "        self.arms_prices = arms_prices\n",
    "        self.arms_bids = arms_bids\n",
    "\n",
    "        self.margin_param = margin_param\n",
    "        self.contexts = contexts\n",
    "\n",
    "        # Learners:\n",
    "        self.context_learners = [UCB_SingleContextLearner(arms_bids, arms_prices, margin_param, context) for context in self.contexts]\n",
    "\n",
    "\n",
    "    def pull_arm(self):\n",
    "        idx_bid_arm_list = []\n",
    "        idx_price_arm_list = []\n",
    "        for context_learner in self.context_learners:\n",
    "            idx_bid_arm, idx_price_arm = context_learner.pull_arm()\n",
    "            idx_bid_arm_list.append(idx_bid_arm)\n",
    "            idx_price_arm_list.append(idx_price_arm)\n",
    "\n",
    "        return (idx_bid_arm_list, idx_price_arm_list, self.contexts)\n",
    "\n",
    "\n",
    "    def update_observations(self, n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list):\n",
    "        # no self.rewards_per_arm[pulled_arm].append(reward) because no concept of an arm (the arms are used inside the context learners)\n",
    "        self.collected_rewards.append(reward_list)\n",
    "        self.collected_n_daily_clicks.append(n_daily_clicks_list)\n",
    "        self.collected_cum_daily_costs.append(cum_daily_costs_list)\n",
    "        self.collected_converted_clicks.append(converted_clicks_list)\n",
    "        self.collected_features.append(features_list)\n",
    "        \n",
    "\n",
    "    def update(self, pulled_arm_bid_list, pulled_arm_price_list,\n",
    "               n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list):\n",
    "        \"\"\"Obs: the length of the lists of the pulled arms are not necessary equal to those in the lists of clicks, costs, rewards and features\"\"\"\n",
    "        self.t += 1\n",
    "        self.update_observations(n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list)\n",
    "\n",
    "        for i,_ in enumerate(self.contexts):\n",
    "            pulled_arm_bid_data = [pulled_arm_bid_list[i]] * len(features_list)\n",
    "            pulled_arm_price_data = [pulled_arm_price_list[i]] * len(features_list)\n",
    "\n",
    "            n_daily_clicks_data = []\n",
    "            cum_daily_costs_data = []\n",
    "            converted_clicks_data = []\n",
    "            reward_data = []\n",
    "            for j,_ in enumerate(features_list):\n",
    "                n_daily_clicks_data.append(n_daily_clicks_list[j])\n",
    "                cum_daily_costs_data.append(cum_daily_costs_list[j])\n",
    "                converted_clicks_data.append(converted_clicks_list[j])\n",
    "                reward_data.append(reward_list[j])\n",
    "\n",
    "            self.context_learners[i].update_from_batch(pulled_arm_bid_data, pulled_arm_price_data,\n",
    "                                                       n_daily_clicks_data, cum_daily_costs_data, converted_clicks_data, reward_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO: to correct\n",
    "# class UCB_ContextsGreadyLearner(UCB_FixedContextsLearner):\n",
    "#     def __init__(self, arms_bids, arms_prices, margin_param, window_context):\n",
    "\n",
    "#         # self.n_features = 2\n",
    "#         contexts = [ [(0,0),(0,1),(1,0),(1,1)] ] # i.e. just one contexts containing all the possible combinations of features\n",
    "#         super().__init__(arms_bids, arms_prices, margin_param, contexts)\n",
    "#         # self.features_partitioned = np.zeros(self.n_features)#notation 0 if that feature is not related to the partition, 1 if it is\n",
    "\n",
    "#         self.window_context = window_context\n",
    "\n",
    "#         # we need\n",
    "#         self.collected_pulled_arm_bid = []# to save the idx of the pulled arms ordered by features not by context\n",
    "#         self.collected_pulled_arm_price = []# to save the idx of the pulled arms ordered by features not by context\n",
    "#         #COM: we need to collect all these informations because they are used when a new context structure is built\n",
    "\n",
    "\n",
    "#     # def pull_arm(self): the same of the super class\n",
    "\n",
    "#     def collect_pulled_arms(self, pulled_arm_bid_list, pulled_arm_price_list, features_list):\n",
    "#         idx_bid_list = []#[[] for fts in features_list]\n",
    "#         idx_price_list =  []#[[] for fts in features_list]\n",
    "\n",
    "#         for fts in features_list:#i,fts in enumerate(features_list):\n",
    "#             for j,context in enumerate(self.contexts):\n",
    "#                 if fts in context:\n",
    "#                     # idx_bid_list[i] = pulled_arm_bid_list[j]\n",
    "#                     # idx_price_list[i] = pulled_arm_price_list[j]\n",
    "#                     idx_bid_list.append(pulled_arm_bid_list[j])\n",
    "#                     idx_price_list.append(pulled_arm_price_list[j])\n",
    "#                     break\n",
    "\n",
    "#         self.collected_pulled_arm_bid.append(idx_bid_list)\n",
    "#         self.collected_pulled_arm_price.append(idx_price_list)\n",
    "\n",
    "\n",
    "#     def get_total_lb_from_contexts(self, context_learners):\n",
    "#         lb_reward_tot = 0\n",
    "#         for context_learner in context_learners:\n",
    "#             lb_clicks = min(context_learner.n_daily_clicks_learner.means - context_learner.n_daily_clicks_learner.sigmas)\n",
    "#             lb_convertion_rate = min(context_learner.conversion_rate_learner.empirical_means - context_learner.conversion_rate_learner.confidence)\n",
    "#             ub_costs = max(context_learner.cum_daily_costs_learner.means + context_learner.cum_daily_costs_learner.sigmas)\n",
    "\n",
    "#             lb_reward = lb_clicks * lb_convertion_rate * self.margin - ub_costs\n",
    "#             lb_reward_tot += lb_reward\n",
    "#         return lb_reward_tot\n",
    "\n",
    "\n",
    "#     def generate_partitions(self):\n",
    "#         \"\"\"Greedy approach:\n",
    "#         returns a list of the partitions to be evaluated\n",
    "#         \"\"\"\n",
    "#         ## General approach if n_features = any number ??\n",
    "#         # something that uses self.features_partitioned and self.contexts to generate the partitions\n",
    "#         # and return the partitions and the indicators of which features are used for those partitions\n",
    "#         #\n",
    "#         # since n_features = 2 it is just:\n",
    "#         partitions_list = []\n",
    "#         if len(self.contexts) == 1:\n",
    "#             partitions_list = [ [[(0,0),(0,1)], [(1,0),(1,1)]], [[(0,0),(1,0)], [(0,1),(1,1)]] ]\n",
    "#         if len(self.contexts) == 2:\n",
    "#             partitions_list = [ [[(0,0)], [(0,1)], [(1,0)], [(1,1)]] ]\n",
    "#         if len(self.contexts) == 4:\n",
    "#             partitions_list = []\n",
    "\n",
    "#         return partitions_list\n",
    "\n",
    "\n",
    "#     def update(self, pulled_arm_bid_list, pulled_arm_price_list,\n",
    "#                n_daily_clicks_list, cum_daily_costs_list, reward_list, features_list):\n",
    "#         \"\"\"Obs: length of the lists of the pulled arms not necessary equal to those in the lists of clicks costs rewards and features\"\"\"\n",
    "\n",
    "#         super().update(pulled_arm_bid_list, pulled_arm_price_list, n_daily_clicks_list, cum_daily_costs_list, reward_list, features_list)\n",
    "#         self.collect_pulled_arms(pulled_arm_bid_list, pulled_arm_price_list, features_list)\n",
    "\n",
    "#         if self.t % self.window_context == 0: #then consider to change the structure of the contexts\n",
    "#             lb_reward_current_partition = self.get_total_lb_from_contexts(self.context_learners)\n",
    "#             partitions_to_evaluate = self.generate_partitions()\n",
    "\n",
    "#             for partition in partitions_to_evaluate:\n",
    "#                 new_context_learners = [UCB_SingleContextLearner(self.arms_bids, self.arms_prices, self.margin, context) for context in partition]\n",
    "\n",
    "#                 for i,context in enumerate(partition):\n",
    "#                     # finding the data to fit the context learner:\n",
    "#                     pulled_arm_bid_data = []\n",
    "#                     pulled_arm_price_data = []\n",
    "#                     n_daily_clicks_data = []\n",
    "#                     cum_daily_costs_data = []\n",
    "#                     reward_data = []\n",
    "#                     for t, collected_features_t in enumerate(self.collected_features):\n",
    "#                         for j, fts in enumerate(collected_features_t):\n",
    "#                             if fts in context:\n",
    "#                                 pulled_arm_bid_data.append(self.collected_pulled_arm_bid[t][j])\n",
    "#                                 pulled_arm_price_data.append(self.collected_pulled_arm_price[t][j])\n",
    "\n",
    "#                                 n_daily_clicks_data.append(self.collected_n_daily_clicks[t][j])\n",
    "#                                 cum_daily_costs_data.append(self.collected_cum_daily_costs[t][j])\n",
    "#                                 reward_data.append(self.collected_rewards[t][j])\n",
    "\n",
    "#                     # fitting the context learner\n",
    "#                     new_context_learners[i].update_from_batch(pulled_arm_bid_data, pulled_arm_price_data,\n",
    "#                                                               n_daily_clicks_data, cum_daily_costs_data, reward_data, self.t)\n",
    "                                                              \n",
    "#                 ## evaluate lower bound of the new partition\n",
    "#                 lb_reward_new_partition = self.get_total_lb_from_contexts(new_context_learners)\n",
    "\n",
    "#                 # Now we need to compare this lb with the lb of the current partition to choose which is better\n",
    "#                 if lb_reward_new_partition > lb_reward_current_partition:\n",
    "#                     self.contexts = partition.copy()\n",
    "#                     self.context_learners = new_context_learners\n",
    "#                     lb_reward_current_partition = lb_reward_new_partition\n",
    "\n",
    "#                     print(f'PARTITION CHANGED AT TIME {self.t}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Com: It is like of the Custom_S3_TS_Learner with the following modifications:\n",
    "\n",
    "Modifications:\n",
    "- new attribute: context (actually, not really necesary)\n",
    "- new method: update_from_batch\n",
    "\"\"\"\n",
    "\n",
    "class TS_SingleContextLearner(Learner):\n",
    "    def __init__(self, arms_bids, arms_prices, margin_param, context):\n",
    "        n_arms_bids = len(arms_bids)\n",
    "        n_arms_prices = len(arms_prices)\n",
    "        \n",
    "        super().__init__(0)\n",
    "        self.rewards_per_arm = [[[] for i in range(n_arms_prices)] for j in range(n_arms_bids)]\n",
    "\n",
    "        self.arms_prices = arms_prices\n",
    "        self.arms_bids = arms_bids\n",
    "        \n",
    "        self.margin_param = margin_param\n",
    "\n",
    "        #learners\n",
    "        self.n_daily_clicks_learner = GPTS_Learner(arms_bids)\n",
    "        self.cum_daily_costs_learner = GPTS_Learner(arms_bids)\n",
    "        self.conversion_rate_learner = Binomial_TS_Learner(n_arms_prices)\n",
    "\n",
    "        self.context = context# it is a list of tuples #\n",
    "        \n",
    "\n",
    "    def pull_arm(self):\n",
    "        sampled_conversion_rates = np.random.beta( self.conversion_rate_learner.beta_parameters[:,0], self.conversion_rate_learner.beta_parameters[:,1] )\n",
    "        sampled_clicks = np.random.normal(self.n_daily_clicks_learner.means, self.n_daily_clicks_learner.sigmas)\n",
    "        sampled_costs = np.random.normal(self.cum_daily_costs_learner.means, self.cum_daily_costs_learner.sigmas)\n",
    "        \n",
    "        idx_price_arm = np.argmax(sampled_conversion_rates * (self.arms_prices - self.margin_param))\n",
    "\n",
    "        idx_bid_arm = np.argmax(\n",
    "            sampled_clicks * sampled_conversion_rates[idx_price_arm] * (self.arms_prices[idx_price_arm] - self.margin_param) - sampled_costs\n",
    "            )\n",
    "        \n",
    "        return idx_bid_arm, idx_price_arm\n",
    "    \n",
    "    \n",
    "    def update_observations(self, pulled_arm_bid, pulled_arm_price, reward):\n",
    "        self.rewards_per_arm[pulled_arm_bid][pulled_arm_price].append(reward)\n",
    "        self.collected_rewards.append(reward)\n",
    "        \n",
    "\n",
    "    def update(self, pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward):\n",
    "        \"\"\"\"\n",
    "        Obs: if n_daily_clicks = 0, we do not update the conversion_rate_learner\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        self.update_observations(pulled_arm_bid, pulled_arm_price, reward)\n",
    "\n",
    "        self.n_daily_clicks_learner.update(pulled_arm_bid, n_daily_clicks)\n",
    "        self.cum_daily_costs_learner.update(pulled_arm_bid, cum_daily_costs)\n",
    "\n",
    "        if n_daily_clicks > 0:\n",
    "            self.conversion_rate_learner.update(pulled_arm_price, converted_clicks, n_daily_clicks)\n",
    "        else:\n",
    "            self.conversion_rate_learner.t += 1# need to be increased, because t is used in the computation of the upperbound\n",
    "            \n",
    "\n",
    "    def update_from_batch(self, pulled_arm_bid_data, pulled_arm_price_data, n_daily_clicks_data, cum_daily_costs_data, converted_clicks_data, reward_data, time):\n",
    "        \"\"\"Obs:\n",
    "            - not all rewards are valid to update the convertion rate, because n_clicks can be 0 in some of values in n_daily_clicks_data\n",
    "        \"\"\"\n",
    "        arms_data = []\n",
    "        k_data = []\n",
    "        k_max_data = []\n",
    "        for i, reward in enumerate(reward_data):\n",
    "            self.update_observations(pulled_arm_bid_data[i], pulled_arm_price_data[i], reward)\n",
    "            if n_daily_clicks_data[i] > 0:\n",
    "                arms_data.append(pulled_arm_price_data[i])\n",
    "                k_data.append(converted_clicks_data[i])\n",
    "                k_max_data.append(n_daily_clicks_data[i])\n",
    "\n",
    "        self.conversion_rate_learner.update_from_batch(arms_data, k_data, k_max_data, time)\n",
    "        self.n_daily_clicks_learner.update_from_batch(pulled_arm_bid_data, n_daily_clicks_data, time)\n",
    "        self.cum_daily_costs_learner.update_from_batch(pulled_arm_bid_data, cum_daily_costs_data, time)\n",
    "        self.t += time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS_FixedContextsLearner(Learner):\n",
    "    def __init__(self, arms_bids, arms_prices, margin_param, contexts):\n",
    "        super().__init__(0)# This learner learns by context and not by arms directly (the arms are inside the context_learners).\n",
    "\n",
    "        self.collected_n_daily_clicks = []\n",
    "        self.collected_cum_daily_costs = []\n",
    "        self.collected_converted_clicks = []\n",
    "        self.collected_features = []\n",
    "\n",
    "        self.n_arms_bids = len(arms_bids)\n",
    "        self.n_arms_prices = len(arms_prices)\n",
    "        self.arms_prices = arms_prices\n",
    "        self.arms_bids = arms_bids\n",
    "\n",
    "        self.margin_param = margin_param\n",
    "        self.contexts = contexts\n",
    "\n",
    "        # Learners:\n",
    "        self.context_learners = [TS_SingleContextLearner(arms_bids, arms_prices, margin_param, context) for context in self.contexts]\n",
    "\n",
    "\n",
    "    def pull_arm(self):\n",
    "        idx_bid_arm_list = []\n",
    "        idx_price_arm_list = []\n",
    "        for context_learner in self.context_learners:\n",
    "            idx_bid_arm, idx_price_arm = context_learner.pull_arm()\n",
    "            idx_bid_arm_list.append(idx_bid_arm)\n",
    "            idx_price_arm_list.append(idx_price_arm)\n",
    "\n",
    "        return idx_bid_arm_list, idx_price_arm_list, self.contexts\n",
    "\n",
    "\n",
    "    def update_observations(self, n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list):\n",
    "        # no self.rewards_per_arm[pulled_arm].append(reward) because no concept of an arm (the arms are used inside the context learners)\n",
    "        self.collected_rewards.append(reward_list)\n",
    "        self.collected_n_daily_clicks.append(n_daily_clicks_list)\n",
    "        self.collected_cum_daily_costs.append(cum_daily_costs_list)\n",
    "        self.collected_converted_clicks.append(converted_clicks_list)\n",
    "        self.collected_features.append(features_list)\n",
    "        \n",
    "\n",
    "    def update(self, pulled_arm_bid_list, pulled_arm_price_list,\n",
    "               n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list):\n",
    "        \"\"\"Obs: the length of the lists of the pulled arms are not necessary equal to those in the lists of clicks, costs, rewards and features\"\"\"\n",
    "        self.t += 1\n",
    "        self.update_observations(n_daily_clicks_list, cum_daily_costs_list, converted_clicks_list, reward_list, features_list)\n",
    "\n",
    "        for i,_ in enumerate(self.contexts):\n",
    "            pulled_arm_bid_data = [pulled_arm_bid_list[i]] * len(features_list)\n",
    "            pulled_arm_price_data = [pulled_arm_price_list[i]] * len(features_list)\n",
    "\n",
    "            n_daily_clicks_data = []\n",
    "            cum_daily_costs_data = []\n",
    "            converted_clicks_data = []\n",
    "            reward_data = []\n",
    "            for j,_ in enumerate(features_list):\n",
    "                n_daily_clicks_data.append(n_daily_clicks_list[j])\n",
    "                cum_daily_costs_data.append(cum_daily_costs_list[j])\n",
    "                converted_clicks_data.append(converted_clicks_list[j])\n",
    "                reward_data.append(reward_list[j])\n",
    "\n",
    "            self.context_learners[i].update_from_batch(pulled_arm_bid_data, pulled_arm_price_data,\n",
    "                                                       n_daily_clicks_data, cum_daily_costs_data, converted_clicks_data, reward_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO\n",
    "\n",
    "# class TS_ContextsGreadyLearner(TS_FixedContextsLearner):\n",
    "#     def __init__(self, arms_bids, arms_prices, margin, window_context):\n",
    "\n",
    "#         # self.n_features = 2\n",
    "#         contexts = [ [(0,0),(0,1),(1,0),(1,1)] ] # i.e. just one contexts containing all the possible combinations of features\n",
    "#         super().__init__(arms_bids, arms_prices, margin, contexts)\n",
    "#         # self.features_partitioned = np.zeros(self.n_features)#notation 0 if that feature is not related to the partition, 1 if it is\n",
    "\n",
    "#         self.window_context = window_context\n",
    "\n",
    "#         # we need\n",
    "#         self.collected_pulled_arm_bid = []# to save the idx of the pulled arms ordered by features not by context\n",
    "#         self.collected_pulled_arm_price = []# to save the idx of the pulled arms ordered by features not by context\n",
    "#         #COM: we need to collect all these informations because they are used when a new context structure is built\n",
    "\n",
    "\n",
    "#     # def pull_arm(self): the same of the super class\n",
    "\n",
    "#     def collect_pulled_arms(self, pulled_arm_bid_list, pulled_arm_price_list, features_list):\n",
    "#         idx_bid_list = []#[[] for fts in features_list]\n",
    "#         idx_price_list =  []#[[] for fts in features_list]\n",
    "\n",
    "#         for fts in features_list:#i,fts in enumerate(features_list):\n",
    "#             for j,context in enumerate(self.contexts):\n",
    "#                 if fts in context:\n",
    "#                     # idx_bid_list[i] = pulled_arm_bid_list[j]\n",
    "#                     # idx_price_list[i] = pulled_arm_price_list[j]\n",
    "#                     idx_bid_list.append(pulled_arm_bid_list[j])\n",
    "#                     idx_price_list.append(pulled_arm_price_list[j])\n",
    "#                     break\n",
    "\n",
    "#         self.collected_pulled_arm_bid.append(idx_bid_list)\n",
    "#         self.collected_pulled_arm_price.append(idx_price_list)\n",
    "\n",
    "\n",
    "#     def get_total_lb_from_contexts(self, context_learners):\n",
    "#         \"\"\"OBS:\n",
    "#             - not really a lb, at least not from the mean and std. It is kind of a sampled lb\n",
    "#         \"\"\"\n",
    "#         lb_reward_tot = 0\n",
    "#         for context_learner in context_learners:\n",
    "#             sampled_conversion_rates = np.random.beta( context_learner.conversion_rate_learner.beta_parameters[:,0], context_learner.conversion_rate_learner.beta_parameters[:,1] )\n",
    "#             sampled_clicks = np.random.normal(context_learner.n_daily_clicks_learner.means, context_learner.n_daily_clicks_learner.sigmas)\n",
    "#             sampled_costs = np.random.normal(context_learner.cum_daily_costs_learner.means, context_learner.cum_daily_costs_learner.sigmas)\n",
    "\n",
    "#             lb_clicks = min(sampled_clicks)\n",
    "#             lb_convertion_rate = min(sampled_conversion_rates)\n",
    "#             ub_costs = max(sampled_costs)\n",
    "\n",
    "#             lb_reward = lb_clicks * lb_convertion_rate * self.margin - ub_costs\n",
    "#             lb_reward_tot += lb_reward\n",
    "#         return lb_reward_tot\n",
    "\n",
    "\n",
    "#     def generate_partitions(self):\n",
    "#         \"\"\"Greedy approach:\n",
    "#         returns a list of the partitions to be evaluated\n",
    "#         \"\"\"\n",
    "#         ## General approach if n_features = any number ??\n",
    "#         # something that uses self.features_partitioned and self.contexts to generate the partitions\n",
    "#         # and return the partitions and the indicators of which features are used for those partitions\n",
    "#         #\n",
    "#         # since n_features = 2 it is just:\n",
    "#         partitions_list = []\n",
    "#         if len(self.contexts) == 1:\n",
    "#             partitions_list = [ [[(0,0),(0,1)], [(1,0),(1,1)]], [[(0,0),(1,0)], [(0,1),(1,1)]] ]\n",
    "#         if len(self.contexts) == 2:\n",
    "#             partitions_list = [ [[(0,0)], [(0,1)], [(1,0)], [(1,1)]] ]\n",
    "#         if len(self.contexts) == 4:\n",
    "#             partitions_list = []\n",
    "\n",
    "#         return partitions_list\n",
    "\n",
    "\n",
    "#     def update(self, pulled_arm_bid_list, pulled_arm_price_list,\n",
    "#                n_daily_clicks_list, cum_daily_costs_list, reward_list, features_list):\n",
    "#         \"\"\"Obs: length of the lists of the pulled arms not necessary equal to those in the lists of clicks costs rewards and features\"\"\"\n",
    "\n",
    "#         super().update(pulled_arm_bid_list, pulled_arm_price_list, n_daily_clicks_list, cum_daily_costs_list, reward_list, features_list)\n",
    "#         self.collect_pulled_arms(pulled_arm_bid_list, pulled_arm_price_list, features_list)\n",
    "\n",
    "#         if self.t % self.window_context == 0: #then consider to change the structure of the contexts\n",
    "#             lb_reward_current_partition = self.get_total_lb_from_contexts(self.context_learners)\n",
    "#             partitions_to_evaluate = self.generate_partitions()\n",
    "\n",
    "#             for partition in partitions_to_evaluate:\n",
    "#                 new_context_learners = [UCB_SingleContextLearner(self.arms_bids, self.arms_prices, self.margin, context) for context in partition]\n",
    "\n",
    "#                 for i,context in enumerate(partition):\n",
    "#                     # finding the data to fit the context learner:\n",
    "#                     pulled_arm_bid_data = []\n",
    "#                     pulled_arm_price_data = []\n",
    "#                     n_daily_clicks_data = []\n",
    "#                     cum_daily_costs_data = []\n",
    "#                     reward_data = []\n",
    "#                     for t, collected_features_t in enumerate(self.collected_features):\n",
    "#                         for j, fts in enumerate(collected_features_t):\n",
    "#                             if fts in context:\n",
    "#                                 pulled_arm_bid_data.append(self.collected_pulled_arm_bid[t][j])\n",
    "#                                 pulled_arm_price_data.append(self.collected_pulled_arm_price[t][j])\n",
    "\n",
    "#                                 n_daily_clicks_data.append(self.collected_n_daily_clicks[t][j])\n",
    "#                                 cum_daily_costs_data.append(self.collected_cum_daily_costs[t][j])\n",
    "#                                 reward_data.append(self.collected_rewards[t][j])\n",
    "\n",
    "#                     # fitting the context learner\n",
    "#                     new_context_learners[i].update_from_batch(pulled_arm_bid_data, pulled_arm_price_data,\n",
    "#                                                               n_daily_clicks_data, cum_daily_costs_data, reward_data, self.t)\n",
    "                                                              \n",
    "#                 ## evaluate lower bound of the new partition\n",
    "#                 lb_reward_new_partition = self.get_total_lb_from_contexts(new_context_learners)\n",
    "\n",
    "#                 # Now we need to compare this lb with the lb of the current partition to choose which is better\n",
    "#                 if lb_reward_new_partition > lb_reward_current_partition:\n",
    "#                     self.contexts = partition.copy()\n",
    "#                     self.context_learners = new_context_learners\n",
    "#                     lb_reward_current_partition = lb_reward_new_partition\n",
    "\n",
    "#                     print(f'PARTITION CHANGED AT TIME {self.t}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 75 #365\n",
    "n_experiments = 25 #50\n",
    "\n",
    "# context parameters:\n",
    "window_context = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0)], [(0, 1)], [(1, 0)]]\n"
     ]
    }
   ],
   "source": [
    "user_classes = [C1, C2, C3]\n",
    "real_contexts = [[(uc.F1, uc.F2)] for uc in user_classes ]\n",
    "print(real_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clairvoyant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User class C0, context [(0, 0)]\n",
      "- optimal bid:  6.0\n",
      "- optimal price:  60\n",
      "\n",
      "- n_daily_clicks on the optimal bid:  100.0\n",
      "- cum_daily_costs on the optimal bid:  360.0\n",
      "- convertion rate on the optimal price:  0.5458775937413701\n",
      "\n",
      "- optimal reward:  731.7551874827402\n",
      "\n",
      "\n",
      "User class C1, context [(0, 1)]\n",
      "- optimal bid:  6.0\n",
      "- optimal price:  50\n",
      "\n",
      "- n_daily_clicks on the optimal bid:  100.0\n",
      "- cum_daily_costs on the optimal bid:  360.0\n",
      "- convertion rate on the optimal price:  0.9\n",
      "\n",
      "- optimal reward:  540.0\n",
      "\n",
      "\n",
      "User class C2, context [(1, 0)]\n",
      "- optimal bid:  6.0\n",
      "- optimal price:  80\n",
      "\n",
      "- n_daily_clicks on the optimal bid:  100.0\n",
      "- cum_daily_costs on the optimal bid:  360.0\n",
      "- convertion rate on the optimal price:  0.8531249999999999\n",
      "\n",
      "- optimal reward:  3052.4999999999995\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt_list = []\n",
    "\n",
    "for i, userclass in enumerate(user_classes):\n",
    "    opt_arm_price = np.argmax(userclass.conversion_rate_function(PRICES) * (PRICES - MARGIN_PARAM))\n",
    "    opt_price = PRICES[opt_arm_price]\n",
    "    opt_conversion_rate = userclass.conversion_rate_function(opt_price)\n",
    "\n",
    "\n",
    "    opt_bid_arm = np.argmax(userclass.n_daily_clicks_function(BIDS) * opt_conversion_rate * (opt_price - MARGIN_PARAM) - userclass.cum_daily_costs_function(BIDS))\n",
    "    opt_bid = BIDS[opt_bid_arm]\n",
    "    opt_n_daily_clicks = userclass.n_daily_clicks_function(opt_bid)\n",
    "    opt_cum_daily_costs = userclass.cum_daily_costs_function(opt_bid)\n",
    "\n",
    "\n",
    "    opt = opt_n_daily_clicks * opt_conversion_rate * (opt_price - MARGIN_PARAM) - opt_cum_daily_costs\n",
    "\n",
    "    opt_list.append(opt)\n",
    "\n",
    "    print(f'User class C{i}, context {real_contexts[i]}')\n",
    "    print(\"- optimal bid: \", opt_bid)\n",
    "    print(\"- optimal price: \", opt_price)\n",
    "    print()\n",
    "    print(\"- n_daily_clicks on the optimal bid: \", opt_n_daily_clicks)\n",
    "    print(\"- cum_daily_costs on the optimal bid: \", opt_cum_daily_costs)\n",
    "    print(\"- convertion rate on the optimal price: \", opt_conversion_rate)\n",
    "    print()\n",
    "    print(\"- optimal reward: \", opt)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: known structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB - known context structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\oswal\\OneDrive - Politecnico di Milano\\Magistrale\\Online Learning Applications\\project\\Online-Learning-Application-Project\\new_approach\\STEP_4.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     pulled_arm_bid, pulled_arm_price, pulled_contexts \u001b[39m=\u001b[39m ucb_learner\u001b[39m.\u001b[39mpull_arm()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     n_daily_clicks, cum_daily_costs, converted_clicks, reward, features \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mround(pulled_arm_bid, pulled_arm_price, pulled_contexts)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     ucb_learner\u001b[39m.\u001b[39;49mupdate(pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward, features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ucb_rewards_per_experiment\u001b[39m.\u001b[39mappend(ucb_learner\u001b[39m.\u001b[39mcollected_rewards)\n",
      "\u001b[1;32mc:\\Users\\oswal\\OneDrive - Politecnico di Milano\\Magistrale\\Online Learning Applications\\project\\Online-Learning-Application-Project\\new_approach\\STEP_4.ipynb Cell 25\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     converted_clicks_data\u001b[39m.\u001b[39mappend(converted_clicks_list[j])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     reward_data\u001b[39m.\u001b[39mappend(reward_list[j])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext_learners[i]\u001b[39m.\u001b[39;49mupdate_from_batch(pulled_arm_bid_data, pulled_arm_price_data,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m                                            n_daily_clicks_data, cum_daily_costs_data, converted_clicks_data, reward_data, \u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\oswal\\OneDrive - Politecnico di Milano\\Magistrale\\Online Learning Applications\\project\\Online-Learning-Application-Project\\new_approach\\STEP_4.ipynb Cell 25\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconversion_rate_learner\u001b[39m.\u001b[39mupdate_from_batch(arms_of_alphas, alphas, time)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_daily_clicks_learner\u001b[39m.\u001b[39mupdate_from_batch(pulled_arm_bid_data, n_daily_clicks_data, time)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcum_daily_costs_learner\u001b[39m.\u001b[39;49mupdate_from_batch(pulled_arm_bid_data, cum_daily_costs_data, time)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oswal/OneDrive%20-%20Politecnico%20di%20Milano/Magistrale/Online%20Learning%20Applications/project/Online-Learning-Application-Project/new_approach/STEP_4.ipynb#X32sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time\n",
      "File \u001b[1;32mc:\\Users\\oswal\\OneDrive - Politecnico di Milano\\Magistrale\\Online Learning Applications\\project\\Online-Learning-Application-Project\\new_approach\\basic_learners.py:200\u001b[0m, in \u001b[0;36mGPUCB_Learner.update_from_batch\u001b[1;34m(self, pulled_arms, rewards, time)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39m# Updating model learner:\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollected_rewards) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgp\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpulled_arms_x,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollected_rewards)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeans, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgp\u001b[39m.\u001b[39mpredict(np\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marms,(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)), return_std\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas, \u001b[39m1e-5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:286\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_marginal_likelihood(theta, clone_kernel\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    283\u001b[0m \u001b[39m# First optimize starting from theta specified in kernel\u001b[39;00m\n\u001b[0;32m    284\u001b[0m optima \u001b[39m=\u001b[39m [\n\u001b[0;32m    285\u001b[0m     (\n\u001b[1;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_constrained_optimization(\n\u001b[0;32m    287\u001b[0m             obj_func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_\u001b[39m.\u001b[39;49mtheta, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_\u001b[39m.\u001b[39;49mbounds\n\u001b[0;32m    288\u001b[0m         )\n\u001b[0;32m    289\u001b[0m     )\n\u001b[0;32m    290\u001b[0m ]\n\u001b[0;32m    292\u001b[0m \u001b[39m# Additional runs are performed from log-uniform chosen initial\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m# theta\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_restarts_optimizer \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:622\u001b[0m, in \u001b[0;36mGaussianProcessRegressor._constrained_optimization\u001b[1;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constrained_optimization\u001b[39m(\u001b[39mself\u001b[39m, obj_func, initial_theta, bounds):\n\u001b[0;32m    621\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfmin_l_bfgs_b\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 622\u001b[0m         opt_res \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49moptimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[0;32m    623\u001b[0m             obj_func,\n\u001b[0;32m    624\u001b[0m             initial_theta,\n\u001b[0;32m    625\u001b[0m             method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    626\u001b[0m             jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    627\u001b[0m             bounds\u001b[39m=\u001b[39;49mbounds,\n\u001b[0;32m    628\u001b[0m         )\n\u001b[0;32m    629\u001b[0m         _check_optimize_result(\u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m, opt_res)\n\u001b[0;32m    630\u001b[0m         theta_opt, func_min \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\scipy\\optimize\\_minimize.py:699\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    697\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    700\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    701\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    702\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    703\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:362\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    356\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    357\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    360\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    363\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    364\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:276\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit.<locals>.obj_func\u001b[1;34m(theta, eval_gradient)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobj_func\u001b[39m(theta, eval_gradient\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    275\u001b[0m     \u001b[39mif\u001b[39;00m eval_gradient:\n\u001b[1;32m--> 276\u001b[0m         lml, grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_marginal_likelihood(\n\u001b[0;32m    277\u001b[0m             theta, eval_gradient\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, clone_kernel\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    278\u001b[0m         )\n\u001b[0;32m    279\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mlml, \u001b[39m-\u001b[39mgrad\n\u001b[0;32m    280\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:609\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.log_marginal_likelihood\u001b[1;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[0;32m    598\u001b[0m inner_term \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m K_inv[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, np\u001b[39m.\u001b[39mnewaxis]\n\u001b[0;32m    599\u001b[0m \u001b[39m# Since we are interested about the trace of\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39m# inner_term @ K_gradient, we don't explicitly compute the\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[39m# matrix-by-matrix operation and instead use an einsum. Therefore\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[39m#             K_gradient[..., param_idx]\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[39m#         )\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m log_likelihood_gradient_dims \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49meinsum(\n\u001b[0;32m    610\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mijl,jik->kl\u001b[39;49m\u001b[39m\"\u001b[39;49m, inner_term, K_gradient\n\u001b[0;32m    611\u001b[0m )\n\u001b[0;32m    612\u001b[0m \u001b[39m# the log likehood gradient is the sum-up across the outputs\u001b[39;00m\n\u001b[0;32m    613\u001b[0m log_likelihood_gradient \u001b[39m=\u001b[39m log_likelihood_gradient_dims\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\oswal\\anaconda3\\envs\\my-env\\lib\\site-packages\\numpy\\core\\einsumfunc.py:1371\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[39mif\u001b[39;00m specified_out:\n\u001b[0;32m   1370\u001b[0m         kwargs[\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m out\n\u001b[1;32m-> 1371\u001b[0m     \u001b[39mreturn\u001b[39;00m c_einsum(\u001b[39m*\u001b[39moperands, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1373\u001b[0m \u001b[39m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[39m# repeat default values here\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m valid_einsum_kwargs \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morder\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcasting\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## UCB learner\n",
    "\n",
    "ucb_rewards_per_experiment = []\n",
    "\n",
    "for e in range(0, n_experiments):\n",
    "    print(f'{e/n_experiments*100}%')\n",
    "\n",
    "    env = Environment(BIDS, PRICES, SIGMA_CLICKS, SIGMA_COSTS, MARGIN_PARAM, user_classes)\n",
    "    ucb_learner = UCB_FixedContextsLearner(BIDS, PRICES, MARGIN_PARAM, real_contexts)\n",
    "\n",
    "    for t in range(0, T):\n",
    "        pulled_arm_bid, pulled_arm_price, pulled_contexts = ucb_learner.pull_arm()\n",
    "        n_daily_clicks, cum_daily_costs, converted_clicks, reward, features = env.round(pulled_arm_bid, pulled_arm_price, pulled_contexts)\n",
    "        ucb_learner.update(pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward, features)\n",
    "\n",
    "    ucb_rewards_per_experiment.append(ucb_learner.collected_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_simulations = np.array(ucb_rewards_per_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    std_inst = np.std(rewards_simulations[:,:,i], axis = 0)/np.sqrt(n_experiments)\n",
    "    std_cum = np.std(np.cumsum(rewards_simulations[:,:,i], axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "    mean_rewards_hat = np.mean(rewards_simulations[:,:,i], axis = 0)\n",
    "    opt = opt_list[i]\n",
    "\n",
    "    #Plots\n",
    "    plt.figure(figsize = (14,6))\n",
    "    plt.suptitle(f'Plots of user class C{i+1}')\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous reward\")\n",
    "    plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "    plt.plot(mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous regret\")\n",
    "    plt.plot(opt - mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative reward\")\n",
    "    plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "    plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative regret\")\n",
    "    plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rewards_simulations = np.sum(rewards_simulations, axis = 2)\n",
    "\n",
    "std_inst = np.std(sum_rewards_simulations, axis = 0)/np.sqrt(n_experiments)\n",
    "std_cum = np.std(np.cumsum(sum_rewards_simulations, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "mean_rewards_hat = np.mean(sum_rewards_simulations, axis = 0)\n",
    "opt = sum(opt_list)\n",
    "\n",
    "\n",
    "#Plots\n",
    "plt.figure(figsize = (14,6))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "plt.plot(mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TS - known context structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_experiments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TS learner\n",
    "\n",
    "ts_rewards_per_experiment = []\n",
    "\n",
    "for e in range(0, n_experiments):\n",
    "    print(f'{e/n_experiments*100}%')\n",
    "\n",
    "    env = Environment(BIDS, PRICES, SIGMA_CLICKS, SIGMA_COSTS, MARGIN_PARAM, user_classes)\n",
    "    ts_learner = TS_FixedContextsLearner(BIDS, PRICES, MARGIN_PARAM, real_contexts)\n",
    "\n",
    "    for t in range(0, T):\n",
    "        pulled_arm_bid, pulled_arm_price, pulled_contexts = ts_learner.pull_arm()\n",
    "        n_daily_clicks, cum_daily_costs, converted_clicks, reward, features = env.round(pulled_arm_bid, pulled_arm_price, pulled_contexts)\n",
    "        ts_learner.update(pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward, features)\n",
    "\n",
    "    ts_rewards_per_experiment.append(ts_learner.collected_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_simulations = np.array(ts_rewards_per_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    std_inst = np.std(rewards_simulations[:,:,i], axis = 0)/np.sqrt(n_experiments)\n",
    "    std_cum = np.std(np.cumsum(rewards_simulations[:,:,i], axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "    mean_rewards_hat = np.mean(rewards_simulations[:,:,i], axis = 0)\n",
    "    opt = opt_list[i]\n",
    "\n",
    "    #Plots\n",
    "    plt.figure(figsize = (14,6))\n",
    "    plt.suptitle(f'Plots of user class C{i+1}')\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous reward\")\n",
    "    plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "    plt.plot(mean_rewards_hat, 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous regret\")\n",
    "    plt.plot(opt - mean_rewards_hat, 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative reward\")\n",
    "    plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "    plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative regret\")\n",
    "    plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rewards_simulations = np.sum(rewards_simulations, axis = 2)\n",
    "\n",
    "std_inst = np.std(sum_rewards_simulations, axis = 0)/np.sqrt(n_experiments)\n",
    "std_cum = np.std(np.cumsum(sum_rewards_simulations, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "mean_rewards_hat = np.mean(sum_rewards_simulations, axis = 0)\n",
    "opt = sum(opt_list)\n",
    "\n",
    "\n",
    "#Plots\n",
    "plt.figure(figsize = (14,6))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "plt.plot(mean_rewards_hat, 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - mean_rewards_hat, 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot both UCB and TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UCB\n",
    "ucb_std_inst = np.std(ucb_rewards_per_experiment, axis = 0) / np.sqrt(n_experiments)\n",
    "ucb_std_cum = np.std(np.cumsum(ucb_rewards_per_experiment, axis = 1), axis = 0) / np.sqrt(n_experiments)\n",
    "\n",
    "ucb_mean_rewards_hat = np.mean(ucb_rewards_per_experiment, axis = 0)\n",
    "\n",
    "## TS\n",
    "ts_std_inst = np.std(ts_rewards_per_experiment, axis = 0)/np.sqrt(n_experiments)\n",
    "ts_std_cum = np.std(np.cumsum(ts_rewards_per_experiment, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "ts_mean_rewards_hat = np.mean(ts_rewards_per_experiment, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting plots\n",
    "plt.figure(figsize = (15,7))\n",
    "\n",
    "#Comparing instantaneous reward\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"opt\")\n",
    "plt.plot(ucb_mean_rewards_hat, color = 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), ucb_mean_rewards_hat - ucb_std_inst, ucb_mean_rewards_hat + ucb_std_inst, alpha = 0.2, color = 'C1')\n",
    "plt.plot(ts_mean_rewards_hat, color = 'C2', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), ts_mean_rewards_hat-ts_std_inst, ts_mean_rewards_hat + ts_std_inst, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing instantaneous regret\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - ucb_mean_rewards_hat, color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), opt - ucb_mean_rewards_hat - ucb_std_inst, opt - ucb_mean_rewards_hat + ucb_std_inst, alpha = 0.2, color = 'C1')\n",
    "plt.plot(opt - ts_mean_rewards_hat, color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), opt - ts_mean_rewards_hat - ts_std_inst, opt - ts_mean_rewards_hat + ts_std_inst, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing cumulative reward\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(ucb_mean_rewards_hat), color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(ucb_mean_rewards_hat) - ucb_std_cum, np.cumsum(ucb_mean_rewards_hat) + ucb_std_cum, alpha = 0.2, color = 'C1')\n",
    "plt.plot(np.cumsum(ts_mean_rewards_hat), color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(ts_mean_rewards_hat) - ts_std_cum, np.cumsum(ts_mean_rewards_hat) + ts_std_cum, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing cumulative regret\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - ucb_mean_rewards_hat), color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(opt - ucb_mean_rewards_hat) - ucb_std_cum, np.cumsum(opt - ucb_mean_rewards_hat) + ucb_std_cum, alpha = 0.2, color = 'C1')\n",
    "plt.plot(np.cumsum(opt - ts_mean_rewards_hat), color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(opt - ts_mean_rewards_hat) - ts_std_cum, np.cumsum(opt - ts_mean_rewards_hat) + ts_std_cum, alpha = 0.2, color = 'C2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: unknown structure & no context generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB - known context structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UCB learner\n",
    "\n",
    "ucb_rewards_per_experiment = []\n",
    "\n",
    "for e in range(0, n_experiments):\n",
    "    print(f'{e/n_experiments*100}%')\n",
    "\n",
    "    env = Environment(BIDS, PRICES, SIGMA_CLICKS, SIGMA_COSTS, MARGIN_PARAM, user_classes)\n",
    "    ucb_learner = UCB_FixedContextsLearner(BIDS, PRICES, MARGIN_PARAM, [ [(0,0),(0,1),(1,0),(1,1)] ])\n",
    "\n",
    "    for t in range(0, T):\n",
    "        pulled_arm_bid, pulled_arm_price, pulled_contexts = ucb_learner.pull_arm()\n",
    "        n_daily_clicks, cum_daily_costs, converted_clicks, reward, features = env.round(pulled_arm_bid, pulled_arm_price, pulled_contexts)\n",
    "        ucb_learner.update(pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward, features)\n",
    "\n",
    "    ucb_rewards_per_experiment.append(ucb_learner.collected_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_simulations = np.array(ucb_rewards_per_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    std_inst = np.std(rewards_simulations[:,:,i], axis = 0)/np.sqrt(n_experiments)\n",
    "    std_cum = np.std(np.cumsum(rewards_simulations[:,:,i], axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "    mean_rewards_hat = np.mean(rewards_simulations[:,:,i], axis = 0)\n",
    "    opt = opt_list[i]\n",
    "\n",
    "    #Plots\n",
    "    plt.figure(figsize = (14,6))\n",
    "    plt.suptitle(f'Plots of user class C{i+1}')\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous reward\")\n",
    "    plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "    plt.plot(mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous regret\")\n",
    "    plt.plot(opt - mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative reward\")\n",
    "    plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "    plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative regret\")\n",
    "    plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "    plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rewards_simulations = np.sum(rewards_simulations, axis = 2)\n",
    "\n",
    "std_inst = np.std(sum_rewards_simulations, axis = 0)/np.sqrt(n_experiments)\n",
    "std_cum = np.std(np.cumsum(sum_rewards_simulations, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "mean_rewards_hat = np.mean(sum_rewards_simulations, axis = 0)\n",
    "opt = sum(opt_list)\n",
    "\n",
    "\n",
    "#Plots\n",
    "plt.figure(figsize = (14,6))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "plt.plot(mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - mean_rewards_hat, 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TS - known context structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_experiments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TS learner\n",
    "\n",
    "ts_rewards_per_experiment = []\n",
    "\n",
    "for e in range(0, n_experiments):\n",
    "    print(f'{e/n_experiments*100}%')\n",
    "\n",
    "    env = Environment(BIDS, PRICES, SIGMA_CLICKS, SIGMA_COSTS, MARGIN_PARAM, user_classes)\n",
    "    ts_learner = TS_FixedContextsLearner(BIDS, PRICES, MARGIN_PARAM, real_contexts)\n",
    "\n",
    "    for t in range(0, T):\n",
    "        pulled_arm_bid, pulled_arm_price, pulled_contexts = ts_learner.pull_arm()\n",
    "        n_daily_clicks, cum_daily_costs, converted_clicks, reward, features = env.round(pulled_arm_bid, pulled_arm_price, pulled_contexts)\n",
    "        ts_learner.update(pulled_arm_bid, pulled_arm_price, n_daily_clicks, cum_daily_costs, converted_clicks, reward, features)\n",
    "\n",
    "    ts_rewards_per_experiment.append(ts_learner.collected_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_simulations = np.array(ts_rewards_per_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    std_inst = np.std(rewards_simulations[:,:,i], axis = 0)/np.sqrt(n_experiments)\n",
    "    std_cum = np.std(np.cumsum(rewards_simulations[:,:,i], axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "    mean_rewards_hat = np.mean(rewards_simulations[:,:,i], axis = 0)\n",
    "    opt = opt_list[i]\n",
    "\n",
    "    #Plots\n",
    "    plt.figure(figsize = (14,6))\n",
    "    plt.suptitle(f'Plots of user class C{i+1}')\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous reward\")\n",
    "    plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "    plt.plot(mean_rewards_hat, 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"instantaneous regret\")\n",
    "    plt.plot(opt - mean_rewards_hat, 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative reward\")\n",
    "    plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "    plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"cumulative regret\")\n",
    "    plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"TS\")\n",
    "    plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rewards_simulations = np.sum(rewards_simulations, axis = 2)\n",
    "\n",
    "std_inst = np.std(sum_rewards_simulations, axis = 0)/np.sqrt(n_experiments)\n",
    "std_cum = np.std(np.cumsum(sum_rewards_simulations, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "mean_rewards_hat = np.mean(sum_rewards_simulations, axis = 0)\n",
    "opt = sum(opt_list)\n",
    "\n",
    "\n",
    "#Plots\n",
    "plt.figure(figsize = (14,6))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"optimal\")\n",
    "plt.plot(mean_rewards_hat, 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), mean_rewards_hat-std_inst, mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - mean_rewards_hat, 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), opt - mean_rewards_hat -std_inst, opt - mean_rewards_hat + std_inst, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(mean_rewards_hat), 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(mean_rewards_hat)-std_cum, np.cumsum(mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - mean_rewards_hat), 'C1', label = \"TS\")\n",
    "plt.fill_between(np.arange(T),np.cumsum(opt - mean_rewards_hat)-std_cum, np.cumsum(opt - mean_rewards_hat) + std_cum, alpha = 0.2, color = 'C1', label = \"1std confidence\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot both UCB and TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UCB\n",
    "ucb_std_inst = np.std(ucb_rewards_per_experiment, axis = 0) / np.sqrt(n_experiments)\n",
    "ucb_std_cum = np.std(np.cumsum(ucb_rewards_per_experiment, axis = 1), axis = 0) / np.sqrt(n_experiments)\n",
    "\n",
    "ucb_mean_rewards_hat = np.mean(ucb_rewards_per_experiment, axis = 0)\n",
    "\n",
    "## TS\n",
    "ts_std_inst = np.std(ts_rewards_per_experiment, axis = 0)/np.sqrt(n_experiments)\n",
    "ts_std_cum = np.std(np.cumsum(ts_rewards_per_experiment, axis = 1), axis = 0)/np.sqrt(n_experiments)\n",
    "\n",
    "ts_mean_rewards_hat = np.mean(ts_rewards_per_experiment, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting plots\n",
    "plt.figure(figsize = (15,7))\n",
    "\n",
    "#Comparing instantaneous reward\n",
    "plt.subplot(2,2,1)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous reward\")\n",
    "plt.plot(np.full(T,opt), label = \"opt\")\n",
    "plt.plot(ucb_mean_rewards_hat, color = 'C1', label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), ucb_mean_rewards_hat - ucb_std_inst, ucb_mean_rewards_hat + ucb_std_inst, alpha = 0.2, color = 'C1')\n",
    "plt.plot(ts_mean_rewards_hat, color = 'C2', label = \"TS\")\n",
    "plt.fill_between(np.arange(T), ts_mean_rewards_hat-ts_std_inst, ts_mean_rewards_hat + ts_std_inst, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing instantaneous regret\n",
    "plt.subplot(2,2,2)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"instantaneous regret\")\n",
    "plt.plot(opt - ucb_mean_rewards_hat, color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), opt - ucb_mean_rewards_hat - ucb_std_inst, opt - ucb_mean_rewards_hat + ucb_std_inst, alpha = 0.2, color = 'C1')\n",
    "plt.plot(opt - ts_mean_rewards_hat, color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), opt - ts_mean_rewards_hat - ts_std_inst, opt - ts_mean_rewards_hat + ts_std_inst, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing cumulative reward\n",
    "plt.subplot(2,2,3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.plot(np.cumsum(np.full(T,opt)), label = \"optimal\")\n",
    "plt.plot(np.cumsum(ucb_mean_rewards_hat), color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(ucb_mean_rewards_hat) - ucb_std_cum, np.cumsum(ucb_mean_rewards_hat) + ucb_std_cum, alpha = 0.2, color = 'C1')\n",
    "plt.plot(np.cumsum(ts_mean_rewards_hat), color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(ts_mean_rewards_hat) - ts_std_cum, np.cumsum(ts_mean_rewards_hat) + ts_std_cum, alpha = 0.2, color = 'C2')\n",
    "plt.legend()\n",
    "\n",
    "#Comparing cumulative regret\n",
    "plt.subplot(2,2,4)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.plot(np.cumsum(opt - ucb_mean_rewards_hat), color = \"C1\", label = \"UCB\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(opt - ucb_mean_rewards_hat) - ucb_std_cum, np.cumsum(opt - ucb_mean_rewards_hat) + ucb_std_cum, alpha = 0.2, color = 'C1')\n",
    "plt.plot(np.cumsum(opt - ts_mean_rewards_hat), color = \"C2\", label = \"TS\")\n",
    "plt.fill_between(np.arange(T), np.cumsum(opt - ts_mean_rewards_hat) - ts_std_cum, np.cumsum(opt - ts_mean_rewards_hat) + ts_std_cum, alpha = 0.2, color = 'C2')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
